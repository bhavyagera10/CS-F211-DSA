{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SourceCodeSummarization.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-sJCPAWXlFS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYWfZYCmDdLL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "e3f100db-2d0a-461a-a2c2-a2c9f612dbed"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tr11eidreHRu",
        "colab_type": "text"
      },
      "source": [
        "Data Generator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OT7FxQzp4Ha7",
        "colab_type": "text"
      },
      "source": [
        "Data Generator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c20TG1Ea494e",
        "colab_type": "text"
      },
      "source": [
        "Data Generator \n",
        "\n",
        "```\n",
        "# In generators\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M87uhrAy5Pda",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "`# This is formatted as code`\n",
        "```\n",
        "\n",
        "# Data Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EFtsfSQX6xn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DataGenerator(object):\n",
        "    \n",
        "    def __init__(self,code_path,commnet_path,test_persentage,max_code_len, max_commnet_len):\n",
        "        self.raw_code = []\n",
        "        self.raw_comment = []\n",
        "\n",
        "        self.code_data = []\n",
        "        self.comment_data = []\n",
        "        \n",
        "        self.train_code_data = []\n",
        "        self.train_comment_data = []\n",
        "        \n",
        "        self.test_code_data = []\n",
        "        self.test_comment_data = []\n",
        "        self.test_raw_comment = []\n",
        "        self.gCodes = []\n",
        "        self.gComment = []\n",
        "        self.index2word = dict()\n",
        "        \n",
        "        self.ReadData(code_path, commnet_path,max_code_len, max_commnet_len)\n",
        "        self.SplitData(test_persentage)\n",
        "        \n",
        "        \n",
        "        self.vocab_size = len(self.index2word)\n",
        "    def getTestData(self):\n",
        "        return self.test_code_data,self.test_comment_data, self.test_raw_comment\n",
        "    def getGdata(self):\n",
        "        return self.gCodes,self.gComment\n",
        "    def ReadData(self, code_path, commnet_path, max_code_len, max_commnet_len):\n",
        "        code_lines=[]\n",
        "        comment_lines=[]\n",
        "        raw_comment_lines = []\n",
        "        with open(code_path) as fin:\n",
        "            code_lines = fin.readlines()\n",
        "        with open(commnet_path) as fin:\n",
        "            comment_lines = fin.readlines()\n",
        "        with open('/content/drive/My Drive/CodeSum/dataset/comment_preproc_F.txt') as fin:\n",
        "            raw_comment_lines = fin.readlines()  #reading pre written comments\n",
        "\n",
        "        gCodes = []\n",
        "        gComment = []\n",
        "        with open('/content/drive/My Drive/CodeSum/dataset/code_f_keyword_indexed.txt') as f: #\n",
        "            for line in f:\n",
        "                gCodes.append([str(c) for c in line.rstrip().split(',')])\n",
        "        with open('/content/drive/My Drive/CodeSum/dataset/comment_f_indexed.txt') as f:\n",
        "            for line in f:\n",
        "                gComment.append([str(c) for c in line.rstrip().split(',')])\n",
        "\n",
        "        for code_line, comment_line, raw_comment_line,code,comment in zip(code_lines,comment_lines,raw_comment_lines,gCodes,gComment):\n",
        "            tokens = code_line.split(\",\")\n",
        "            words = comment_line.split(\",\")\n",
        "            if len(tokens) > max_code_len or len(words) > max_commnet_len:\n",
        "                continue\n",
        "            if len(tokens) <= 12 or len(words) <= 3:\n",
        "                continue\n",
        "            tokens = [int(token) for token in tokens]\n",
        "            self.code_data.append(tokens)\n",
        "            words = [int(token) for token in words]\n",
        "            self.comment_data.append(words)\n",
        "\n",
        "            self.raw_comment.append(raw_comment_line)\n",
        "            self.gCodes.append(code)\n",
        "            self.gComment.append(comment)\n",
        "        np.random.seed(30)\n",
        "        np.random.shuffle(self.code_data)\n",
        "        \n",
        "        np.random.seed(30)\n",
        "        np.random.shuffle(self.comment_data)\n",
        "\n",
        "        np.random.seed(30)\n",
        "        np.random.shuffle(self.raw_comment)\n",
        "        np.random.seed(30)\n",
        "        np.random.shuffle(self.gCodes)\n",
        "        np.random.seed(30)\n",
        "        np.random.shuffle(self.gComment)\n",
        "        print (\"num of data:\",len(self.code_data))\n",
        "        with open(\"/content/drive/My Drive/CodeSum/dataset/comment_f_keyword_Vocab.txt\") as fin: #removing stopwords from pre written comments\n",
        "            for i, word in enumerate(fin):\n",
        "                self.index2word[i] = word.rstrip() #assigning index to words\n",
        "    def SplitData(self,test_persentage):\n",
        "        \n",
        "        self.train_code_data = self.code_data[:int(len(self.code_data) * (1 - test_persentage))]\n",
        "        self.train_comment_data = self.comment_data[:int(len(self.comment_data) * (1 - test_persentage))]\n",
        "        \n",
        "        self.test_code_data = self.code_data[int(len(self.code_data) * (1 - test_persentage)):]\n",
        "        self.test_comment_data = self.comment_data[int(len(self.comment_data) * (1 - test_persentage)):]\n",
        "        self.test_raw_comment = self.raw_comment[int(len(self.raw_comment) * (1 - test_persentage)):]\n",
        "\n",
        "        self.gCodes = self.gCodes[int(len(self.gCodes) * (1 - test_persentage)):]\n",
        "        self.gComment = self.gComment[int(len(self.gComment) * (1 - test_persentage)):]\n",
        "\n",
        "    def MakeDataset(self, train):\n",
        "\n",
        "        codes = []\n",
        "        keywords = []\n",
        "\n",
        "        if train:\n",
        "            codes = self.train_code_data\n",
        "            keywords = self.train_comment_data\n",
        "\n",
        "        else:\n",
        "            codes = self.test_code_data\n",
        "            keywords = self.test_comment_data\n",
        "\n",
        "        train_codes = []\n",
        "        train_keywords = []\n",
        "\n",
        "        next_words = np.zeros((len(keywords), self.vocab_size), dtype=np.bool)\n",
        "\n",
        "        for i, (code, keyword) in enumerate(zip(codes, keywords)):\n",
        "            train_codes.append(code)\n",
        "            keyword = set(keyword)\n",
        "            for index in keyword:\n",
        "                next_words[i,index] = 1\n",
        "\n",
        "        return train_codes, next_words\n",
        "                "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQAT92zSZ-UH",
        "colab_type": "text"
      },
      "source": [
        "Training model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYCn788GOwHI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-Y6-kYEh75h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 768
        },
        "outputId": "0d125e09-4a93-4438-b143-d8b1ff27aa86"
      },
      "source": [
        "!pip install tensorflow\n",
        "!pip install keras"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.6/dist-packages (2.3.0)\n",
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.4.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.10.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.3.3)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.35.1)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.12.4)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.31.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.1)\n",
            "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.8.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.18.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.9.2->tensorflow) (49.6.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (0.4.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (3.2.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.7.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.17.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow) (1.7.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.1.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.6)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (3.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (2.4.3)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras) (2.10.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras) (1.4.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras) (3.13)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras) (1.18.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->keras) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhy7eRbYePIA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7f75aebd-4813-4e83-95f1-caa0a91037f7"
      },
      "source": [
        "import os\n",
        "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "#import tensorflow as tf\n",
        "import tensorflow.compat.v1 as tf\n",
        "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.2)\n",
        "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, allow_soft_placement=True, log_device_placement=True))\n",
        "\n",
        "#from groundhog.trainer import SGD\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.models import Sequential\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.layers.convolutional import Convolution1D\n",
        "from keras.layers.core import Activation, Flatten, Dense, Dropout, RepeatVector\n",
        "from keras.layers.pooling import MaxPooling1D\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.layers.recurrent import LSTM\n",
        "from keras.layers.wrappers import TimeDistributed\n",
        "#from keras.engine.topology import Merge\n",
        "from keras.preprocessing import sequence\n",
        "import time\n",
        "from keras import callbacks\n",
        "from skimage.feature.tests.test_util import plt\n",
        "import keras\n",
        "\n",
        "#import data_generator\n",
        "\n",
        "\n",
        "max_caption_len = 20\n",
        "maxlen = 600\n",
        "\n",
        "data_gen = DataGenerator(\"/content/drive/My Drive/CodeSum/dataset/code_f_keyword_indexed.txt\", \"/content/drive/My Drive/CodeSum/dataset/comment_f_keyword_indexed.txt\", 0.20,\n",
        "                                        maxlen, max_caption_len)\n",
        "\n",
        "codes, next_words = data_gen.MakeDataset(train=True)\n",
        "#\n",
        "codes = sequence.pad_sequences(codes, maxlen=maxlen)\n",
        "\n",
        "\n",
        "codesT, next_wordsT = data_gen.MakeDataset(train=False)\n",
        "codesT = sequence.pad_sequences(codesT, maxlen=maxlen)\n",
        "\n",
        "\n",
        "vocab_size = 4000\n",
        "\n",
        "max_features = 6000  # size of token\n",
        "\n",
        "nb_output = data_gen.vocab_size  # len(Y_train[0])\n",
        "\n",
        "nb_filter = 128\n",
        "filter_length = 10\n",
        "hidden_dims = 256\n",
        "embedding_dims = 128\n",
        "\n",
        "code_model = Sequential()\n",
        "code_model.add(Embedding(max_features, embedding_dims, input_length=maxlen))\n",
        "code_model.add(Dropout(0.2))\n",
        "#def Convolution1D(filters, kernel_size, strides=1, padding='valid', data_format='channels_last', dilation_rate=1, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, **kwargs)\n",
        "code_model.add(Convolution1D(filters=nb_filter,kernel_size=10,padding='valid',dilation_rate=1))\n",
        "code_model.add(Activation('relu'))\n",
        "\n",
        "code_model.add(Convolution1D(filters=nb_filter,kernel_size=5,padding='valid',dilation_rate=1))\n",
        "code_model.add(Activation('relu'))\n",
        "\n",
        "code_model.add(Convolution1D(filters=nb_filter, kernel_size=3,padding='valid',dilation_rate=1))\n",
        "code_model.add(Activation('relu'))\n",
        "#MaxPooling1D(pool_size=2, strides=None, padding='valid', data_format='channels_last', **kwargs)\n",
        "\n",
        "code_model.add(MaxPooling1D(pool_size=code_model.output_shape[1]))\n",
        "code_model.add(Flatten())\n",
        "# We add a vanilla hidden layer:\n",
        "code_model.add(Dense(256))\n",
        "code_model.add(Activation('relu'))\n",
        "\n",
        "code_model.add(Dropout(0.5))\n",
        "code_model.add(Dense(nb_output))\n",
        "code_model.add(Activation('sigmoid'))\n",
        "\n",
        "code_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
        "\n",
        "\n",
        "print (\"train\")#\n",
        "early = callbacks.EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='auto')\n",
        "cp = keras.callbacks.ModelCheckpoint('/content/drive/My Drive/CodeSum/dataset/keyword_f.h5', monitor='val_loss', verbose=0, save_best_only=True,\n",
        "                                     save_weights_only=True, mode='auto')\n",
        "\n",
        "del data_gen\n",
        "\n",
        "code_model.fit(codes, next_words, batch_size=32, epochs=500, validation_data=(codesT, next_wordsT), callbacks=[early])\n",
        "\n",
        "json_string = code_model.to_json()\n",
        "open('/content/drive/My Drive/CodeSum/dataset/keyword_f.json', 'w').write(json_string)\n",
        "code_model.save_weights('/content/drive/My Drive/CodeSum/dataset/keyword_f.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device mapping:\n",
            "/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n",
            "\n",
            "num of data: 25072\n",
            "train\n",
            "Epoch 1/500\n",
            "627/627 [==============================] - 509s 812ms/step - loss: 0.0219 - accuracy: 0.1791 - val_loss: 0.0098 - val_accuracy: 0.1870\n",
            "Epoch 2/500\n",
            "627/627 [==============================] - 512s 817ms/step - loss: 0.0101 - accuracy: 0.1810 - val_loss: 0.0097 - val_accuracy: 0.1844\n",
            "Epoch 3/500\n",
            "627/627 [==============================] - 514s 820ms/step - loss: 0.0099 - accuracy: 0.2609 - val_loss: 0.0094 - val_accuracy: 0.3428\n",
            "Epoch 4/500\n",
            "627/627 [==============================] - 514s 820ms/step - loss: 0.0095 - accuracy: 0.3801 - val_loss: 0.0092 - val_accuracy: 0.4098\n",
            "Epoch 5/500\n",
            "627/627 [==============================] - 515s 821ms/step - loss: 0.0092 - accuracy: 0.4030 - val_loss: 0.0088 - val_accuracy: 0.4225\n",
            "Epoch 6/500\n",
            "627/627 [==============================] - 512s 817ms/step - loss: 0.0089 - accuracy: 0.4163 - val_loss: 0.0086 - val_accuracy: 0.4375\n",
            "Epoch 7/500\n",
            "627/627 [==============================] - 515s 821ms/step - loss: 0.0087 - accuracy: 0.4243 - val_loss: 0.0084 - val_accuracy: 0.4445\n",
            "Epoch 8/500\n",
            "627/627 [==============================] - 520s 829ms/step - loss: 0.0084 - accuracy: 0.4396 - val_loss: 0.0081 - val_accuracy: 0.4572\n",
            "Epoch 9/500\n",
            "627/627 [==============================] - 511s 814ms/step - loss: 0.0081 - accuracy: 0.4470 - val_loss: 0.0079 - val_accuracy: 0.4596\n",
            "Epoch 10/500\n",
            "627/627 [==============================] - 516s 822ms/step - loss: 0.0079 - accuracy: 0.4460 - val_loss: 0.0078 - val_accuracy: 0.4688\n",
            "Epoch 11/500\n",
            "627/627 [==============================] - 513s 819ms/step - loss: 0.0076 - accuracy: 0.4492 - val_loss: 0.0076 - val_accuracy: 0.4648\n",
            "Epoch 12/500\n",
            "627/627 [==============================] - 516s 823ms/step - loss: 0.0074 - accuracy: 0.4520 - val_loss: 0.0074 - val_accuracy: 0.4742\n",
            "Epoch 13/500\n",
            "627/627 [==============================] - 513s 818ms/step - loss: 0.0072 - accuracy: 0.4519 - val_loss: 0.0073 - val_accuracy: 0.4708\n",
            "Epoch 14/500\n",
            "627/627 [==============================] - 509s 811ms/step - loss: 0.0070 - accuracy: 0.4557 - val_loss: 0.0073 - val_accuracy: 0.4642\n",
            "Epoch 15/500\n",
            "627/627 [==============================] - 512s 816ms/step - loss: 0.0069 - accuracy: 0.4495 - val_loss: 0.0072 - val_accuracy: 0.4658\n",
            "Epoch 16/500\n",
            "627/627 [==============================] - 511s 816ms/step - loss: 0.0067 - accuracy: 0.4523 - val_loss: 0.0071 - val_accuracy: 0.4582\n",
            "Epoch 17/500\n",
            "627/627 [==============================] - 511s 816ms/step - loss: 0.0066 - accuracy: 0.4513 - val_loss: 0.0071 - val_accuracy: 0.4654\n",
            "Epoch 18/500\n",
            "627/627 [==============================] - 511s 816ms/step - loss: 0.0064 - accuracy: 0.4481 - val_loss: 0.0070 - val_accuracy: 0.4702\n",
            "Epoch 19/500\n",
            "627/627 [==============================] - 514s 821ms/step - loss: 0.0063 - accuracy: 0.4489 - val_loss: 0.0069 - val_accuracy: 0.4582\n",
            "Epoch 20/500\n",
            "627/627 [==============================] - 515s 822ms/step - loss: 0.0062 - accuracy: 0.4455 - val_loss: 0.0069 - val_accuracy: 0.4554\n",
            "Epoch 21/500\n",
            "627/627 [==============================] - 513s 819ms/step - loss: 0.0061 - accuracy: 0.4452 - val_loss: 0.0069 - val_accuracy: 0.4638\n",
            "Epoch 22/500\n",
            "627/627 [==============================] - 510s 814ms/step - loss: 0.0060 - accuracy: 0.4484 - val_loss: 0.0068 - val_accuracy: 0.4670\n",
            "Epoch 23/500\n",
            "627/627 [==============================] - 506s 808ms/step - loss: 0.0059 - accuracy: 0.4436 - val_loss: 0.0068 - val_accuracy: 0.4612\n",
            "Epoch 24/500\n",
            "627/627 [==============================] - 508s 811ms/step - loss: 0.0058 - accuracy: 0.4444 - val_loss: 0.0069 - val_accuracy: 0.4646\n",
            "Epoch 25/500\n",
            "627/627 [==============================] - 507s 809ms/step - loss: 0.0057 - accuracy: 0.4461 - val_loss: 0.0069 - val_accuracy: 0.4528\n",
            "Epoch 26/500\n",
            "627/627 [==============================] - 511s 816ms/step - loss: 0.0057 - accuracy: 0.4389 - val_loss: 0.0067 - val_accuracy: 0.4526\n",
            "Epoch 27/500\n",
            "627/627 [==============================] - 510s 813ms/step - loss: 0.0056 - accuracy: 0.4418 - val_loss: 0.0067 - val_accuracy: 0.4560\n",
            "Epoch 28/500\n",
            "627/627 [==============================] - 517s 824ms/step - loss: 0.0055 - accuracy: 0.4397 - val_loss: 0.0068 - val_accuracy: 0.4618\n",
            "Epoch 29/500\n",
            "627/627 [==============================] - 521s 831ms/step - loss: 0.0055 - accuracy: 0.4414 - val_loss: 0.0068 - val_accuracy: 0.4596\n",
            "Epoch 30/500\n",
            "627/627 [==============================] - 512s 817ms/step - loss: 0.0054 - accuracy: 0.4353 - val_loss: 0.0069 - val_accuracy: 0.4560\n",
            "Epoch 31/500\n",
            "627/627 [==============================] - 511s 815ms/step - loss: 0.0054 - accuracy: 0.4335 - val_loss: 0.0068 - val_accuracy: 0.4596\n",
            "Epoch 32/500\n",
            "627/627 [==============================] - 519s 828ms/step - loss: 0.0053 - accuracy: 0.4365 - val_loss: 0.0068 - val_accuracy: 0.4491\n",
            "Epoch 33/500\n",
            "627/627 [==============================] - 517s 824ms/step - loss: 0.0053 - accuracy: 0.4373 - val_loss: 0.0069 - val_accuracy: 0.4548\n",
            "Epoch 34/500\n",
            "627/627 [==============================] - 524s 836ms/step - loss: 0.0052 - accuracy: 0.4346 - val_loss: 0.0068 - val_accuracy: 0.4514\n",
            "Epoch 35/500\n",
            "627/627 [==============================] - 528s 843ms/step - loss: 0.0052 - accuracy: 0.4363 - val_loss: 0.0067 - val_accuracy: 0.4516\n",
            "Epoch 36/500\n",
            "627/627 [==============================] - 529s 844ms/step - loss: 0.0051 - accuracy: 0.4352 - val_loss: 0.0067 - val_accuracy: 0.4590\n",
            "Epoch 00036: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Gp_YlZcX7TL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "74f8aa94-d202-4cf5-8c2f-e2487bb455bf"
      },
      "source": [
        "import os\n",
        "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "#import tensorflow as tf\n",
        "import tensorflow.compat.v1 as tf\n",
        "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.2)\n",
        "#gpu_options=tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.2)\n",
        "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, allow_soft_placement=True, log_device_placement=True))\n",
        "#from groundhog.trainer import SGD\n",
        "#from tensorflow import keras\n",
        "from tensorflow.python.keras.layers.normalization import BatchNormalization\n",
        "from tensorflow.python.keras.models import Sequential\n",
        "from tensorflow.python.keras.layers.embeddings import Embedding\n",
        "from tensorflow.python.keras.layers.convolutional import Convolution1D\n",
        "from tensorflow.python.keras.layers.core import Activation, Flatten, Dense, Dropout, RepeatVector\n",
        "from tensorflow.python.keras.layers.pooling import MaxPooling1D\n",
        "from tensorflow.python.keras.optimizers import RMSprop\n",
        "from tensorflow.python.keras.layers.recurrent import LSTM\n",
        "from tensorflow.python.keras.layers.wrappers import TimeDistributed\n",
        "#from tensorflow.python.keras.layers.core import Merge\n",
        "from tensorflow.python.keras.preprocessing import sequence\n",
        "#import keras\n",
        "import time\n",
        "from tensorflow.python.keras import callbacks\n",
        "from skimage.feature.tests.test_util import plt\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device mapping:\n",
            "/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HTIDQaVHTaQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "99dee7df-9a20-4d1b-dbcd-173907f854a6"
      },
      "source": [
        "!pip install groundhog.trainer"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement groundhog.trainer (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for groundhog.trainer\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84Nt79FQHo3X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f28f6c84-9097-4f94-9b75-55cc99467c79"
      },
      "source": [
        "!python setup.py develop"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "python3: can't open file 'setup.py': [Errno 2] No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeOjby8_X7Mf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_caption_len = 20\n",
        "maxlen = 600"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZRNnsZmX7FR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "db487a48-a672-4a1c-f1dd-b5197a09739c"
      },
      "source": [
        "data_gen = DataGenerator(\"/content/drive/My Drive/CodeSum/dataset/code_f_keyword_indexed.txt\", \"/content/drive/My Drive/CodeSum/dataset/comment_f_keyword_indexed.txt\", 0.20, maxlen, max_caption_len)\n",
        "#giving files with words from code and comment converted to index also same word/ token has same index\n",
        "codes, next_words = data_gen.MakeDataset(train=True)\n",
        "#\n",
        "codes = sequence.pad_sequences(codes, maxlen=maxlen)\n",
        "\n",
        "\n",
        "codesT, next_wordsT = data_gen.MakeDataset(train=False)\n",
        "codesT = sequence.pad_sequences(codesT, maxlen=maxlen)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "num of data: 25072\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d47lgCO4rVCQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size = 4000\n",
        "\n",
        "max_features = 6000  # size of token\n",
        "\n",
        "nb_output = data_gen.vocab_size  # len(Y_train[0])\n",
        "\n",
        "nb_filter = 128\n",
        "filter_length = 10\n",
        "hidden_dims = 256\n",
        "embedding_dims = 128"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56lFIEZs03hO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "outputId": "da18edcc-59b4-4863-dd9f-c692f8915d99"
      },
      "source": [
        "!pip install keras==1.0.1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras==1.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ce/14/84c9d7ed1093cb0ad215073bd1fd4c3c910b5ac7aea36766fdb903efb277/Keras-1.0.1.tar.gz (109kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: theano in /usr/local/lib/python3.6/dist-packages (from keras==1.0.1) (1.0.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==1.0.1) (3.13)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from keras==1.0.1) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from theano->keras==1.0.1) (1.18.5)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from theano->keras==1.0.1) (1.4.1)\n",
            "Building wheels for collected packages: keras\n",
            "  Building wheel for keras (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras: filename=Keras-1.0.1-cp36-none-any.whl size=132078 sha256=b647bbd70c61b7e667fc4838379db7681d11e9370b9acdce8598da8feec956e8\n",
            "  Stored in directory: /root/.cache/pip/wheels/e9/4d/8e/3964c381b769bb3286d75c98e0e0c9c78fd63821e7018c0b36\n",
            "Successfully built keras\n",
            "\u001b[31mERROR: textgenrnn 1.4.1 has requirement keras>=2.1.5, but you'll have keras 1.0.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: kapre 0.1.3.1 has requirement keras>=2.0.0, but you'll have keras 1.0.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fancyimpute 0.4.3 has requirement keras>=2.0.0, but you'll have keras 1.0.1 which is incompatible.\u001b[0m\n",
            "Installing collected packages: keras\n",
            "  Found existing installation: Keras 2.4.3\n",
            "    Uninstalling Keras-2.4.3:\n",
            "      Successfully uninstalled Keras-2.4.3\n",
            "Successfully installed keras-1.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzNJcEXjb3pM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "code_model = Sequential()\n",
        "code_model.add(Embedding(max_features, embedding_dims, input_length=maxlen))\n",
        "code_model.add(Dropout(0.2))\n",
        "#def Convolution1D(filters, kernel_size, strides=1, padding='valid', data_format='channels_last', dilation_rate=1, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, **kwargs)\n",
        "code_model.add(Convolution1D(filters=nb_filter,kernel_size=10,padding='valid',dilation_rate=1))\n",
        "code_model.add(Activation('relu'))\n",
        "\n",
        "code_model.add(Convolution1D(filters=nb_filter,kernel_size=5,padding='valid',dilation_rate=1))\n",
        "code_model.add(Activation('relu'))\n",
        "\n",
        "code_model.add(Convolution1D(filters=nb_filter, kernel_size=3,padding='valid',dilation_rate=1))\n",
        "code_model.add(Activation('relu'))\n",
        "#MaxPooling1D(pool_size=2, strides=None, padding='valid', data_format='channels_last', **kwargs)\n",
        "\n",
        "code_model.add(MaxPooling1D(pool_size=code_model.output_shape[1]))\n",
        "code_model.add(Flatten())\n",
        "# We add a vanilla hidden layer:\n",
        "code_model.add(Dense(256))\n",
        "code_model.add(Activation('relu'))\n",
        "\n",
        "code_model.add(Dropout(0.5))\n",
        "code_model.add(Dense(nb_output))\n",
        "code_model.add(Activation('sigmoid'))\n",
        "\n",
        "code_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLbM-49g7jy0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-I_7DRCKb3yz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "507f6b19-73f8-4930-b7a8-7a345c3b3d3c"
      },
      "source": [
        "print (\"train\")\n",
        "early = callbacks.EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='auto')\n",
        "cp = tf.keras.callbacks.ModelCheckpoint('/content/drive/My Drive/CodeSum/dataset/keyword_f.h5', monitor='val_loss', verbose=0, save_best_only=True,\n",
        "                                     save_weights_only=True, mode='auto')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGUvgbyBb312",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "f72ba755-aff5-472c-8bcc-67d4527515fb"
      },
      "source": [
        "del data_gen\n",
        "\n",
        "code_model.fit(codes, next_words, batch_size=32, epochs=500, validation_data=(codesT, next_wordsT), callbacks=[early])\n",
        "\n",
        "json_string = code_model.to_json()\n",
        "open('/content/drive/My Drive/CodeSum/dataset/keyword_f.json', 'w').write(json_string)\n",
        "code_model.save_weights('/content/drive/My Drive/CodeSum/dataset/keyword_f.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-329f0e9d14c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mdata_gen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcode_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcodesT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_wordsT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mearly\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mjson_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcode_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'data_gen' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWo0UU93cNt0",
        "colab_type": "text"
      },
      "source": [
        "Keyword Predictor "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBOm8kFKb34v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "17fe447a-b546-476b-9aaf-610bb9f5e2da"
      },
      "source": [
        "import os\n",
        "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '2'\n",
        "import tensorflow.compat.v1 as tf\n",
        "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.2)\n",
        "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, allow_soft_placement=True, log_device_placement=True))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device mapping:\n",
            "/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Uurf4nab38L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.python.keras.models import model_from_json, Sequential\n",
        "from tensorflow.python.keras.preprocessing import sequence\n",
        "import numpy as np\n",
        "import math\n",
        "import logging\n",
        "import sys\n",
        "import time\n",
        "import nltk\n",
        "#import data_generator\n",
        "from scipy.stats.mstats_basic import sen_seasonal_slopes\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYWNGlkKb4T0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logging.basicConfig(level=logging.INFO, stream=sys.stdout)\n",
        "logger = logging.getLogger(__name__)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3Ueoh4eb4Jf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SentenceGeneration(object):\n",
        "    def __init__(self):\n",
        "        self.model = Sequential()\n",
        "        self.index2word = dict()\n",
        "        self.word2Index = dict()\n",
        "        self.index2token = dict()\n",
        "        self.token2Index = dict()\n",
        "        with open(\"/content/drive/My Drive/CodeSum/dataset/comment_f_keyword_Vocab.txt\") as fin:\n",
        "            for i, word in enumerate(fin):\n",
        "                word = word.rstrip()             #saving indexes to words and vise-versa\n",
        "                self.index2word[i] = word\n",
        "                self.word2Index[word] = int(i)\n",
        "        with open(\"/content/drive/My Drive/CodeSum/dataset/code_f_keyword_Vocab.txt\") as fin:\n",
        "            for i, word in enumerate(fin):\n",
        "                word = word.rstrip()            #saving indexes to tokens and\n",
        "                self.index2token[i] = word\n",
        "                self.token2Index[word] = int(i)\n",
        "\n",
        "    # def __init__(self,codes,targets):\n",
        "    #     self.model = Sequential()\n",
        "    #     # self.codes = codes\n",
        "    #     # self.targets = targets\n",
        "\n",
        "    def readModel(self, name):\n",
        "        model = model_from_json(open('/content/drive/My Drive/CodeSum/dataset/'+name + '.json').read())\n",
        "        model.load_weights('/content/drive/My Drive/CodeSum/dataset/'+name + '.h5')\n",
        "        model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=[\"accuracy\"])\n",
        "\n",
        "        self.model = model\n",
        "\n",
        "    def printSentence(self, indices):\n",
        "        return ' '.join([self.index2word[x] for x in indices])\n",
        "\n",
        "        #for index in indices:\n",
        "        #    print self.index2word[index],\n",
        "    def returnCode(self, indices):\n",
        "\n",
        "        return ' '.join([self.index2token[x-1] for x in indices])\n",
        "\n",
        "    def removeToken(self,indices):\n",
        "        indices = indices[1:-1]\n",
        "        unk_index = self.word2Index['UNK']\n",
        "        if unk_index in indices:\n",
        "            indices.remove(unk_index)\n",
        "        return indices\n",
        "\n",
        "    def generateSentence(self, code, n):\n",
        "\n",
        "        code = sequence.pad_sequences([code], maxlen=600)\n",
        "\n",
        "\n",
        "        pred = self.model.predict(code)[0]\n",
        "        sorted_pred = np.argsort(pred)[::-1]\n",
        "\n",
        "        pred_word = [ self.index2token[index] for index in sorted_pred[:n]]\n",
        "        return pred_word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wu0RalqFb4II",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jom0bXTWb4BD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rAtk(pred, target, k):\n",
        "    #sorted_pred = np.argsort(pred)[::-1]\n",
        "    correct = 0\n",
        "    for i in pred[:k]:\n",
        "        if i in target:\n",
        "            correct += 1\n",
        "\n",
        "    return correct / float(len(target))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nbv5ra85b3_p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "49424c2d-0216-4ffd-9253-a63687009d95"
      },
      "source": [
        "gen = SentenceGeneration()\n",
        "\n",
        "\n",
        "data_gen = DataGenerator(\"/content/drive/My Drive/CodeSum/dataset/code_f_keyword_indexed.txt\", \"/content/drive/My Drive/CodeSum/dataset/comment_f_keyword_indexed.txt\",0.20, 600, 20)\n",
        "gen.readModel('keyword_f')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "num of data: 25072\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YLLUCKfdcED",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "codes,keywords,raw_comment= data_gen.getTestData()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33CcBtAgdcOy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(30)\n",
        "np.random.shuffle(codes)\n",
        "np.random.seed(30)\n",
        "np.random.shuffle(raw_comment)\n",
        "np.random.seed(30)\n",
        "np.random.shuffle(keywords)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaFEJeYqdvl4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sens = []\n",
        "co = []\n",
        "comm = []\n",
        "r = 0\n",
        "k = 30"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUdN-wnxdvwb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2693d4ca-6d61-4276-ba9e-da31a9f68bd2"
      },
      "source": [
        "for i,(code,comment) in enumerate(zip(codes,keywords)):\n",
        "    #keyword = gen.generateSentence(code,5)\n",
        "    c = sequence.pad_sequences([code], maxlen=600)\n",
        "    pred = gen.model.predict(c)[0]\n",
        "    #pred[52] = 0\n",
        "\n",
        "    sorted_pred = np.argsort(pred)[::-1]\n",
        "\n",
        "    sens.append([gen.index2word[s] for s in sorted_pred[:k]])\n",
        "    co.append(gen.returnCode(code))\n",
        "    comm.append(gen.printSentence(comment))\n",
        "\n",
        "    r += rAtk(sorted_pred, comment, k)\n",
        "\n",
        "\n",
        "\n",
        "    sys.stdout.write('\\r' + str(i)+' score :'+str(r/(i+1)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5014 score :0.650133887800147"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejLPRZTqd4hp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('/content/drive/My Drive/CodeSum/dataset/keyword_prediction4.txt', 'w') as fin:\n",
        "    fin.write(str(r/(i+1)))\n",
        "    fin.write(\"\\n\")\n",
        "    for i in range(len(co[:300])):\n",
        "        s = str(co[i])\n",
        "        code = s.replace(\"{\",\"{\\n\").replace(\";\",\";\\n\").replace(\"}\",\"}\\n\")\n",
        "        fin.write(\"code:\\n\"+ code)\n",
        "        fin.write(\"comment:\\n\"+ raw_comment[i].rstrip()+ '\\n')\n",
        "        fin.write(\"--generate--\\n\")\n",
        "\n",
        "        fin.write(' '.join(sens[i])+'\\n')\n",
        "        fin.write(\"\\n\")\n",
        "    # for sen in sens:\n",
        "    #     fin.write(gen.printSentence(sen[1:-1])+'\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcPfvk6meMqb",
        "colab_type": "text"
      },
      "source": [
        "MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C33Kvp5UfcCU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.python.keras.models import Sequential\n",
        "from tensorflow.python.keras.layers.embeddings import Embedding\n",
        "from tensorflow.python.keras.layers.convolutional import Convolution1D\n",
        "from tensorflow.python.keras.layers.core import Activation, Flatten, Dense, Dropout, RepeatVector\n",
        "from tensorflow.python.keras.layers.pooling import MaxPooling1D\n",
        "from tensorflow.python.keras.optimizers import RMSprop, Adam\n",
        "from tensorflow.python.keras.layers.recurrent import LSTM\n",
        "from tensorflow.python.keras.layers.wrappers import TimeDistributed\n",
        "#from tensorflow.python.keras.engine.topology import Merge\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "db3NO9BkfcTV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import logging\n",
        "import sys\n",
        "from tensorflow.python.keras.regularizers import l2\n",
        "from tensorflow.python.keras.engine.training import Model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCvdbvvsfcdE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logging.basicConfig(level=logging.INFO, stream=sys.stdout)\n",
        "logger = logging.getLogger(__name__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRoVAtTZfc3O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CGModel(object):\n",
        "    '''\n",
        "    classdocs\n",
        "    '''\n",
        "\n",
        "    #def __init__(self, max_features=20000, code_maxlen=600, embed_size=100, hidden_size=200, vocab_size=4000 , optimiser, weights=None, gru=False, clipnorm=-1, batch_size=None,t=None, lr=0.001):\n",
        "    def __init__(self, max_features, code_maxlen, embed_size, hidden_size, vocab_size , optimiser, weights, gru, clipnorm, batch_size,t, lr):\n",
        "        \n",
        "        self.max_t = t  # Expected timesteps. Needed to build the Theano graph\n",
        "\n",
        "        # Model hyperparameters\n",
        "        self.max_features = 20000\n",
        "        self.vocab_size = vocab_size  # size of word vocabulary\n",
        "        self.embed_size = 100  # number of units in a word embedding\n",
        "        self.hidden_size = 200 # number of units in first LSTM\n",
        "\n",
        "        # Optimiser hyperparameters\n",
        "        self.optimiser = optimiser  # optimisation method\n",
        "        self.lr = lr\n",
        "        self.beta1 = 0.9\n",
        "        self.beta2 = 0.999\n",
        "        self.epsilon = 1e-8\n",
        "        self.clipnorm = clipnorm\n",
        "\n",
        "        self.weights = weights  # initialise with checkpointed weights?\n",
        "        \n",
        "        \n",
        "        \n",
        "\n",
        "\n",
        "    def buildModel(self):\n",
        "        '''\n",
        "        Define the exact structure of your model here. We create an image\n",
        "        description generation model by merging the VGG image features with\n",
        "        a word embedding model, with an LSTM over the sequences.\n",
        "        '''\n",
        "        logger.info('Building Keras model...')\n",
        "\n",
        "                \n",
        "        \n",
        "        code_model = Sequential()\n",
        "        code_model.add(Embedding(self.max_features, self.embed_size, input_length=maxlen))#,dropout=0.2))\n",
        "     #    Convolution1D(filters, kernel_size, strides=1, padding='valid', data_format='channels_last', dilation_rate=1, groups=1, \n",
        "    #activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', \n",
        "    #kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, **kwargs)\n",
        "        code_model.add(Convolution1D(filters=nb_filter,kernel_size=filter_length, padding='valid',  groups=1))\n",
        "        #model.add(BatchNormalization())\n",
        "        code_model.add(Activation('relu'))\n",
        "        \n",
        "        code_model.add(Convolution1D(filters=nb_filter,kernel_size=filter_length,padding='valid',groups=1))\n",
        "        # model.add(BatchNormalization())\n",
        "        code_model.add(Activation('relu'))\n",
        "        \n",
        "        code_model.add(MaxPooling1D(pool_size=code_model.output_shape[1]))\n",
        "        code_model.add(Flatten())\n",
        "        # We add a vanilla hidden layer:\n",
        "        code_model.add(Dense(128))\n",
        "        \n",
        "        \n",
        "        # next, let's define a RNN model that encodes sequences of words\n",
        "        # into sequences of 128-dimensional word vectors.\n",
        "        language_model = Sequential()\n",
        "        language_model.add(Embedding(vocab_size, 256, input_length=max_caption_len))\n",
        "        language_model.add(LSTM(128, return_sequences=True))\n",
        "        language_model.add(TimeDistributed(Dense(128)))\n",
        "        \n",
        "        # let's repeat the image vector to turn it into a sequence.\n",
        "        code_model.add(RepeatVector(max_caption_len))\n",
        "        \n",
        "        # the output of both models will be tensors of shape (samples, max_caption_len, 128).\n",
        "        # let's concatenate these 2 vector sequences.\n",
        "        model = Sequential()\n",
        "        #x = tf.keras.layers.Concatenate(axis=-1) (inputs=[t,h,mem_encoder])\n",
        "        model.add(tf.keras.layers.Concatenate(axis =-1)(inputs=[code_model , language_model ]) )\n",
        "        # let's encode this vector sequence into a single vector\n",
        "        model.add(LSTM(256, return_sequences=False))\n",
        "        # which will be used to compute a probability\n",
        "        # distribution over what the next word in the caption should be!\n",
        "        model.add(Dense(self.vocab_size))\n",
        "        model.add(Activation('softmax'))\n",
        "        \n",
        "        model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=[\"accuracy\"])\n",
        "        \n",
        "        \n",
        "        #model.fit([codes, partial_captions], next_words, batch_size=16, nb_epoch=100)\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAdqoj-0Pao1",
        "colab_type": "text"
      },
      "source": [
        "**dropout check pls, groups or dilation rate**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-IMnFILBn-Y5",
        "colab_type": "text"
      },
      "source": [
        "Generating Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqDrRpLMfck8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from tensorflow.python.keras.models import model_from_json, Sequential\n",
        "from tensorflow.python.keras.preprocessing import sequence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LTDjLEDfciR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SentenceGeneration(object):\n",
        "    def __init__(self):\n",
        "        self.model = Sequential()\n",
        "        self.index2word = dict()\n",
        "        self.word2Index = dict()\n",
        "        self.index2token = dict()\n",
        "        self.token2Index = dict()\n",
        "        with open(\"/content/drive/My Drive/CodeSum/dataset/comment_f_Vocab.txt\") as fin:\n",
        "            for i, word in enumerate(fin):\n",
        "                word = word.rstrip()\n",
        "                self.index2word[i] = word\n",
        "                self.word2Index[word] = int(i)\n",
        "        with open(\"/content/drive/My Drive/CodeSum/dataset/code_f_keyword_Vocab.txt\") as fin:\n",
        "            for i, word in enumerate(fin):\n",
        "                word = word.rstrip()\n",
        "                self.index2token[i] = word\n",
        "                self.token2Index[word] = int(i)\n",
        "\n",
        "    # def __init__(self,codes,targets):\n",
        "    #     self.model = Sequential()\n",
        "    #     # self.codes = codes\n",
        "    #     # self.targets = targets\n",
        "\n",
        "    def readModel(self, name):\n",
        "        model = model_from_json(open('/content/drive/My Drive/CodeSum/dataset/'+name + '.json').read())\n",
        "        model.load_weights('/content/drive/My Drive/CodeSum/dataset/'+name + '.h5')\n",
        "        model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=[\"accuracy\"])\n",
        "\n",
        "        self.model = model\n",
        "\n",
        "    def printSentence(self, indices):\n",
        "        return ' '.join([self.index2word[x] for x in indices])\n",
        "\n",
        "        #for index in indices:\n",
        "        #    print self.index2word[index],\n",
        "    def returnCode(self, indices):\n",
        "\n",
        "        return ' '.join([self.index2token[x-1] for x in indices])\n",
        "\n",
        "    def removeToken(self,indices):\n",
        "        indices = indices[1:-1]\n",
        "        unk_index = self.word2Index['UNK']\n",
        "        if unk_index in indices:\n",
        "            indices.remove(unk_index)\n",
        "        return indices\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def generateSentence(self, code, n):\n",
        "\n",
        "        code = sequence.pad_sequences([code], maxlen=600)\n",
        "\n",
        "\n",
        "        pred = self.model.predict(code)[0]\n",
        "        sorted_pred = np.argsort(pred)[::-1]\n",
        "\n",
        "        pred_word = [ self.index2token[index] for index in sorted_pred[:n]]\n",
        "        return pred_word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2TiLY_Btfcgd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "023f72d6-818f-42a8-9c05-7a5a208aac41"
      },
      "source": [
        "class DataGenerator(object):\n",
        "    def __init__(self, code_path, commnet_path, test_persentage, max_code_len, max_commnet_len):\n",
        "        self.raw_code = []\n",
        "        self.raw_comment = []\n",
        "\n",
        "        self.code_split_plag = []\n",
        "\n",
        "        self.code_data = []\n",
        "        self.comment_data = []\n",
        "\n",
        "        self.train_code_data = []\n",
        "        self.train_comment_data = []\n",
        "\n",
        "        self.valid_code_data = []\n",
        "        self.valid_comment_data = []\n",
        "\n",
        "        self.test_code_data = []\n",
        "        self.test_comment_data = []\n",
        "        self.test_raw_comment = []\n",
        "\n",
        "        self.index2word = dict()\n",
        "\n",
        "        self.ReadData(code_path, commnet_path, max_code_len, max_commnet_len)\n",
        "        self.SplitData(test_persentage)\n",
        "\n",
        "        self.vocab_size = len(self.index2word)\n",
        "\n",
        "    def getTestData(self):\n",
        "        return self.test_code_data, self.test_comment_data, self.test_raw_comment\n",
        "\n",
        "    def getTestID(self):\n",
        "        with open('/content/drive/My Drive/CodeSum/codenn/data/stackoverflow/csharp/dev/codenn.txt') as f:\n",
        "            ids = [val.split('\\t')[0] for val in f]\n",
        "        return ids\n",
        "  #data_gen = DataGenerator(\"/content/drive/My Drive/CodeSum/dataset/code_f_keyword_indexed.txt\", \"/content/drive/My Drive/CodeSum/dataset/comment_f_indexed.txt\", 0.20,maxlen, max_caption_len)\n",
        "    def ReadData(self, code_path, commnet_path, max_code_len, max_commnet_len):\n",
        "        code_lines = []\n",
        "        comment_lines = []\n",
        "        raw_comment_lines = []\n",
        "        code_split_plags = []\n",
        "        with open(code_path) as fin:\n",
        "            code_lines = fin.readlines()\n",
        "        with open(commnet_path) as fin:\n",
        "            comment_lines = fin.readlines()\n",
        "        with open('/content/drive/My Drive/CodeSum/dataset/comment_preproc_F.txt') as fin:\n",
        "            raw_comment_lines = fin.readlines()\n",
        "\n",
        "        with open('/content/drive/My Drive/CodeSum/dataset/code_f_keyword_Vocab.txt') as fin: #getting eerrors\n",
        "            code_split_plags = fin.readlines()\n",
        "            #qnaSet\n",
        "#error line error line\n",
        "        count = 0\n",
        "        for code_line, comment_line, raw_comment_line, plag in zip(code_lines, comment_lines, raw_comment_lines,\n",
        "                                                                   code_split_plags):\n",
        "            tokens = code_line.split(\",\")\n",
        "            words = comment_line.split(\",\")\n",
        "            # if len(tokens) > max_code_len or len(words) > max_commnet_len:\n",
        "            #    continue\n",
        "\n",
        "            if (len(tokens) <= 12 or len(words) <= 3) and plag.rstrip() != 'test':\n",
        "                continue\n",
        "\n",
        "            tokens = [int(token) for token in tokens]\n",
        "            self.code_data.append(tokens)\n",
        "            words = [int(token) for token in words]\n",
        "            self.comment_data.append(words)\n",
        "\n",
        "            self.raw_comment.append(raw_comment_line)\n",
        "            self.code_split_plag.append(str(plag).rstrip())\n",
        "\n",
        "        # np.random.seed(30)\n",
        "        # np.random.shuffle(self.code_data)\n",
        "\n",
        "        # np.random.seed(30)\n",
        "        # np.random.shuffle(self.comment_data)\n",
        "\n",
        "        # np.random.seed(30)\n",
        "        # np.random.shuffle(self.raw_comment)\n",
        "        print (\"num of data:\", len(self.code_data))\n",
        "\n",
        "        with open(\"/content/drive/My Drive/CodeSum/dataset/comment_f_Vocab.txt\") as fin:\n",
        "            for i, word in enumerate(fin):\n",
        "                self.index2word[i] = word.rstrip()\n",
        "\n",
        "    def SplitData(self, test_persentage):\n",
        "\n",
        "        for code, comment, rawComment, flag in zip(self.code_data, self.comment_data, self.raw_comment,\n",
        "                                                   self.code_split_plag):\n",
        "            if flag == \"train\":\n",
        "                self.train_code_data.append(code)\n",
        "                self.train_comment_data.append(comment)\n",
        "            elif flag == \"valid\":\n",
        "                self.valid_code_data.append(code)\n",
        "                self.valid_comment_data.append(comment)\n",
        "            else:\n",
        "                self.test_code_data.append(code)\n",
        "                self.test_comment_data.append(comment)\n",
        "                self.test_raw_comment.append(rawComment)\n",
        "\n",
        "\n",
        "    def MakeDataset(self,train,divide=1,part=0):\n",
        "\n",
        "        codes = []\n",
        "        captions = []\n",
        "\n",
        "        if train == True:\n",
        "            c = len(self.train_code_data)/divide\n",
        "            if divide == part+1:\n",
        "                codes = self.train_code_data[c*part:]\n",
        "                captions = self.train_comment_data[c*part:]\n",
        "            else:\n",
        "                codes = self.train_code_data[c * part:c * (part+1)]\n",
        "                captions = self.train_comment_data[c * part:c * (part+1)]\n",
        "        else:\n",
        "            codes = self.test_code_data\n",
        "            captions = self.test_comment_data\n",
        "\n",
        "        train_codes = []\n",
        "        patial_captions = []\n",
        "        next_words = []\n",
        "\n",
        "\n",
        "        for code, caption in zip(codes,captions):\n",
        "            for i in range(1,len(caption)):\n",
        "                next_word = [0 for j in range(self.vocab_size)]\n",
        "                patial_caption = caption[:i]\n",
        "                next_word[int(caption[i])] = 1\n",
        "\n",
        "                train_codes.append(code)\n",
        "                patial_captions.append(patial_caption)\n",
        "                next_words.append(next_word)\n",
        "\n",
        "        return train_codes, patial_captions, next_words\n",
        "\n",
        "    def MakeDataset3(self, train, divide=1, part=0):\n",
        "\n",
        "        codes = []\n",
        "        captions = []\n",
        "\n",
        "        if train:\n",
        "            codes = self.train_code_data\n",
        "            captions = self.train_comment_data\n",
        "\n",
        "        else:\n",
        "            codes = self.test_code_data\n",
        "            captions = self.test_comment_data\n",
        "\n",
        "        train_codes = []\n",
        "        patial_captions = []\n",
        "        next_words = []\n",
        "\n",
        "        for code, caption in zip(codes, captions):\n",
        "            for i in range(1, len(caption)):\n",
        "                patial_caption = caption[:i]\n",
        "                next_words.append(int(caption[i]))\n",
        "                train_codes.append(code)\n",
        "                patial_captions.append(patial_caption)\n",
        "\n",
        "\n",
        "\n",
        "        next_words2 = np.zeros((len(next_words), self.vocab_size), dtype=np.bool)\n",
        "        for i,index in enumerate(next_words):\n",
        "            next_words2[i,index] = 1\n",
        "\n",
        "        return train_codes, patial_captions, next_words2\n",
        "\n",
        "    def MakeDataset4(self, train, mem_size):\n",
        "\n",
        "        codes = []\n",
        "        captions = []\n",
        "\n",
        "        if train:\n",
        "            codes = self.train_code_data\n",
        "            captions = self.train_comment_data\n",
        "\n",
        "        else:\n",
        "            codes = self.test_code_data\n",
        "            captions = self.test_comment_data\n",
        "\n",
        "        train_codes = []\n",
        "        patial_captions = []\n",
        "        next_words = []\n",
        "\n",
        "        gen = SentenceGeneration()\n",
        "       #error coming here #gen.readModel('qnaData_all_keyword')\n",
        "        gen.readModel('keyword_f')\n",
        "        predict_keywords = []\n",
        "\n",
        "        for code, caption in zip(codes, captions):\n",
        "            c = sequence.pad_sequences([code], maxlen=500)\n",
        "            pred = gen.model.predict(c)[0]\n",
        "\n",
        "            sorted_pred = np.argsort(pred)[::-1]\n",
        "\n",
        "            for i in range(1, len(caption)):\n",
        "                patial_caption = caption[:i]\n",
        "                next_words.append(int(caption[i]))\n",
        "\n",
        "                train_codes.append(code)\n",
        "                patial_captions.append(patial_caption)\n",
        "                predict_keywords.append([s for s in sorted_pred[:mem_size]])\n",
        "\n",
        "        next_words2 = np.zeros((len(next_words), self.vocab_size), dtype=np.bool)\n",
        "        for i,index in enumerate(next_words):\n",
        "            next_words2[i, index] = 1\n",
        "        return train_codes, patial_captions, next_words2, predict_keywords\n",
        "\n",
        "    def MakeDataset2(self, train):\n",
        "        codes = []\n",
        "        captions = []\n",
        "\n",
        "        if train:\n",
        "\n",
        "            codes = self.train_code_data\n",
        "            captions = self.train_comment_data\n",
        "        else:\n",
        "            codes = self.test_code_data\n",
        "            captions = self.test_comment_data\n",
        "\n",
        "        train_codes = []\n",
        "        patial_captions = []\n",
        "\n",
        "\n",
        "        for j,(code, caption) in enumerate(zip(codes, captions)):\n",
        "            for i in range(1, len(caption)):\n",
        "\n",
        "\n",
        "                patial_caption = caption[:i]\n",
        "\n",
        "                train_codes.append(code)\n",
        "                patial_captions.append(patial_caption)\n",
        "\n",
        "        next_words = np.zeros((len(train_codes), self.vocab_size), dtype=np.bool)\n",
        "\n",
        "        for j,(code, caption) in enumerate(zip(codes, captions)):\n",
        "            for i in range(1, len(caption)):\n",
        "\n",
        "                next_words[j, int(caption[i])] = 1\n",
        "\n",
        "        return train_codes, patial_captions, next_words\n",
        "\n",
        "     \n",
        "                \n",
        "                \n",
        "             \n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "X_train = X_data[:int(len(X_data) * (1 - 0.25))]\n",
        "Y_train = Y_data[:int(len(X_data) * (1 - 0.25))]\n",
        "X_test = X_data[int(len(X_data) * (1 - 0.25)):]\n",
        "Y_test = Y_data[int(len(X_data) * (1 - 0.25)):]\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nX_train = X_data[:int(len(X_data) * (1 - 0.25))]\\nY_train = Y_data[:int(len(X_data) * (1 - 0.25))]\\nX_test = X_data[int(len(X_data) * (1 - 0.25)):]\\nY_test = Y_data[int(len(X_data) * (1 - 0.25)):]\\nX_train = sequence.pad_sequences(X_train, maxlen=maxlen)\\nX_test = sequence.pad_sequences(X_test, maxlen=maxlen)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfYe1073UJWC",
        "colab_type": "text"
      },
      "source": [
        "**#gen.readModel('qnaData_all_keyword')**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRInnNT3TOcr",
        "colab_type": "text"
      },
      "source": [
        "**qnaSet dataset not found**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HXX46ZEoyun",
        "colab_type": "text"
      },
      "source": [
        "Training Attmem2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlmVcZICpI2H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "5199c6b1-4c38-4782-96c1-b5b1a1b06837"
      },
      "source": [
        "import os\n",
        "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "import tensorflow.compat.v1 as tf\n",
        "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.8)\n",
        "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, allow_soft_placement=True, log_device_placement=True))\n",
        "\n",
        "from tensorflow.python.keras.models import Sequential\n",
        "from tensorflow.python.keras.layers.convolutional import Convolution1D\n",
        "from tensorflow.python.keras.layers.core import Activation, Flatten, Dense, Dropout, RepeatVector\n",
        "from tensorflow.python.keras.layers.pooling import MaxPooling1D\n",
        "from tensorflow.python.keras.optimizers import RMSprop\n",
        "from tensorflow.python.keras.layers import Input, LSTM, Embedding, Dense\n",
        "\n",
        "#import data_generator\n",
        "from tensorflow.python.keras.preprocessing import sequence\n",
        "import keras\n",
        "\n",
        "from tensorflow.python.keras.models import Model\n",
        "\n",
        "#from bleu import computeMaps, bleuFromMaps\n",
        "#from text_generator import SentenceGeneration"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device mapping:\n",
            "/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWuWTrT6pJea",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_caption_len = 26\n",
        "maxlen = 500\n",
        "mem_size = 30\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yH3Orm52pJot",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a6be7d89-4cd8-4b86-db91-3c78a935d068"
      },
      "source": [
        "data_gen = DataGenerator(\"/content/drive/My Drive/CodeSum/dataset/code_f_keyword_indexed.txt\", \"/content/drive/My Drive/CodeSum/dataset/comment_f_indexed.txt\", 0.20,maxlen, max_caption_len)\n",
        "\n",
        "codes, partial_captions, next_words, predict_words= data_gen.MakeDataset4(train=True, mem_size=mem_size)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "num of data: 4885\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1I6OSzJpJyf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "codes = sequence.pad_sequences(codes, maxlen=maxlen)\n",
        "partial_captions = sequence.pad_sequences(partial_captions, maxlen=max_caption_len)\n",
        "predict_words = sequence.pad_sequences(predict_words, maxlen=mem_size)\n",
        "\n",
        "codesT, partial_captionsT, next_wordsT, predict_wordsT = data_gen.MakeDataset4(train=False, mem_size=mem_size)\n",
        "codesT = sequence.pad_sequences(codesT, maxlen=maxlen)\n",
        "partial_captionsT = sequence.pad_sequences(partial_captionsT, maxlen=max_caption_len)\n",
        "predict_wordsT = sequence.pad_sequences(predict_wordsT, maxlen=mem_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45Mgr6RBpn6W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "d3440560-803d-4fcd-be99-69cab587b09f"
      },
      "source": [
        "vocab_size = 5000\n",
        "\n",
        "max_features = 10000  # size of token\n",
        "\n",
        "nb_output = data_gen.vocab_size  # len(Y_train[0])\n",
        "\n",
        "nb_filter = 128\n",
        "filter_length = 5\n",
        "hidden_dims = 256\n",
        "embedding_dims = 128\n",
        "\n",
        "mem_size = 30"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-c094715d8646>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmax_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10000\u001b[0m  \u001b[0;31m# size of token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mnb_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_gen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m  \u001b[0;31m# len(Y_train[0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mnb_filter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'data_gen' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUvi7zSxd8W9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwgClFp7poHA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "outputId": "ed7b512c-edce-4cd3-b7fc-ddf77121e700"
      },
      "source": [
        "embedding_for_nse = tf.compat.v1.keras.layers.Embedding(vocab_size,128,Dropout(0.3),input_length=max_caption_len)\n",
        "#embedding_for_nse(Dropout(0.3))\n",
        "#def tf.compat.v1.keras.layers.Embedding(input_dim, output_dim, embeddings_initializer='uniform', \n",
        "#embeddings_regularizer=None, activity_regularizer=None, embeddings_constraint=None, mask_zero=False, \n",
        "#input_length=None, **kwargs)\n",
        "\n",
        "language_input = tf.compat.v1.keras.Input(shape=(max_caption_len,), dtype= 'float32')\n",
        "print(language_input)\n",
        "language_embedding = embedding_for_nse(language_input)\n",
        "vocab_input = Input(shape=(mem_size,), dtype= 'float32')\n",
        "vocab_embedding = embedding_for_nse(vocab_input)\n",
        "\n",
        "\n",
        "match = tf.keras.layers.Dot(axes=[2,2])([language_embedding,vocab_embedding])#,dot_axes=[2,2])\n",
        "match = Activation('softmax')(match)\n",
        "\n",
        "mem_encoder = tf.keras.layers.Dot(axes=[2,1])(inputs=[match,vocab_embedding])#,dot_axes=[2,1])\n",
        "mem_encoder = Activation('relu')(mem_encoder) # mem : memory embedding\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"input_11:0\", shape=(None, 20), dtype=float32)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-65-90c15af942a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mlanguage_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_caption_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mlanguage_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding_for_nse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mvocab_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmem_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mvocab_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding_for_nse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    924\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_in_functional_construction_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m       return self._functional_construction_call(inputs, args, kwargs,\n\u001b[0;32m--> 926\u001b[0;31m                                                 input_list)\n\u001b[0m\u001b[1;32m    927\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m     \u001b[0;31m# Maintains info about the `Layer.call` stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_functional_construction_call\u001b[0;34m(self, inputs, args, kwargs, input_list)\u001b[0m\n\u001b[1;32m   1096\u001b[0m         \u001b[0;31m# Build layer if applicable (if the `build` method has been\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1097\u001b[0m         \u001b[0;31m# overridden).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m         \u001b[0mcast_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_maybe_build\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2641\u001b[0m         \u001b[0;31m# operations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2642\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_init_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2643\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2644\u001b[0m       \u001b[0;31m# We must set also ensure that the layer is marked as built, and the build\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2645\u001b[0m       \u001b[0;31m# shape is stored since user defined build functions may not be calling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(instance, input_shape)\u001b[0m\n\u001b[1;32m    321\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput_shape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m       \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_shapes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_tuples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m     \u001b[0moutput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m     \u001b[0;31m# Return shapes from `fn` as TensorShapes.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0moutput_shape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/embeddings.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    147\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'embeddings'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m           \u001b[0mregularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings_regularizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m           constraint=self.embeddings_constraint)\n\u001b[0m\u001b[1;32m    150\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36madd_weight\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, trainable, constraint, partitioner, use_resource, synchronization, aggregation, **kwargs)\u001b[0m\n\u001b[1;32m    612\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 614\u001b[0;31m         caching_device=caching_device)\n\u001b[0m\u001b[1;32m    615\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mregularizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m       \u001b[0;31m# TODO(fchollet): in the future, this should be handled at the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_add_variable_with_custom_getter\u001b[0;34m(self, name, shape, dtype, initializer, getter, overwrite, **kwargs_for_getter)\u001b[0m\n\u001b[1;32m    748\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m         \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 750\u001b[0;31m         **kwargs_for_getter)\n\u001b[0m\u001b[1;32m    751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m     \u001b[0;31m# If we set an initializer and the variable processed it, tracking will not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer_utils.py\u001b[0m in \u001b[0;36mmake_variable\u001b[0;34m(name, shape, dtype, initializer, trainable, caching_device, validate_shape, constraint, use_resource, collections, synchronization, aggregation, partitioner)\u001b[0m\n\u001b[1;32m    143\u001b[0m       \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m       \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m       shape=variable_shape if variable_shape else None)\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    258\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mVariableV1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_v1_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_v2_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m_variable_v1_call\u001b[0;34m(cls, initial_value, trainable, collections, validate_shape, caching_device, name, variable_def, dtype, expected_shape, import_scope, constraint, use_resource, synchronization, aggregation, shape)\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m         shape=shape)\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m   def _variable_v2_call(cls,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m                         shape=None):\n\u001b[1;32m    198\u001b[0m     \u001b[0;34m\"\"\"Call on Variable class. Useful to force the signature.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m     \u001b[0mprevious_getter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdefault_variable_creator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgetter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creator_stack\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m       \u001b[0mprevious_getter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_getter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_getter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mdefault_variable_creator\u001b[0;34m(next_creator, **kwargs)\u001b[0m\n\u001b[1;32m   2595\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2596\u001b[0m         \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2597\u001b[0;31m         shape=shape)\n\u001b[0m\u001b[1;32m   2598\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2599\u001b[0m     return variables.RefVariable(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    262\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_v2_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariableMetaclass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, variable_def, import_scope, constraint, distribute_strategy, synchronization, aggregation, shape)\u001b[0m\n\u001b[1;32m   1516\u001b[0m           \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m           \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m           distribute_strategy=distribute_strategy)\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m   def _init_from_args(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36m_init_from_args\u001b[0;34m(self, initial_value, trainable, collections, caching_device, name, dtype, constraint, synchronization, aggregation, distribute_strategy, shape)\u001b[0m\n\u001b[1;32m   1649\u001b[0m           \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Initializer\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1650\u001b[0m             initial_value = ops.convert_to_tensor(\n\u001b[0;32m-> 1651\u001b[0;31m                 \u001b[0minitial_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minit_from_fn\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0minitial_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1652\u001b[0m                 name=\"initial_value\", dtype=dtype)\n\u001b[1;32m   1653\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    983\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_auto_cast_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: call() got an unexpected keyword argument 'dtype'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LfET0MeepoRT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# the output of both models will be tensors of shape (samples, max_caption_len, 128).\n",
        "# let's encode this vector sequence into a single vector\n",
        "h = LSTM(400, return_sequences=True)(language_embedding)\n",
        "# which will be used to compute a probability\n",
        "# distribution over what the next word in the caption should be!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lImvtvQKpodN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "outputId": "86bbe65d-6f16-45b3-e201-76aa3d5e6f76"
      },
      "source": [
        "code_input = Input(shape=(maxlen,),dtype='float32')\n",
        "print(code_input)\n",
        "embedding_for_code = Embedding(max_features, 400, maxlen,Dropout(0.5))(code_input)\n",
        "#embedding_for_code=(Dropout(0.5))\n",
        "code_match = tf.keras.layers.Dot(axes=[2,2]) (inputs=[h,embedding_for_code])#,dot_axes=[2,2])\n",
        "code_match = Activation('softmax')(code_match)\n",
        "code_attention = tf.keras.layers.Dot(axes=[2,1])(inputs=[code_match,embedding_for_code])#,dot_axes=[2,1])\n",
        "code_attention = Activation('relu')(code_attention) # mem : memory embedding\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"input_10:0\", shape=(None, 500), dtype=float32)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-62-dbf64e205112>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcode_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0membedding_for_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m400\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m#embedding_for_code=(Dropout(0.5))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcode_match\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0membedding_for_code\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#,dot_axes=[2,2])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/embeddings.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dim, output_dim, embeddings_initializer, embeddings_regularizer, activity_regularizer, embeddings_constraint, mask_zero, input_length, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings_initializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitializers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings_initializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings_regularizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregularizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings_regularizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivity_regularizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregularizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivity_regularizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/initializers/__init__.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(identifier)\u001b[0m\n\u001b[1;32m    160\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     raise ValueError('Could not interpret initializer identifier: ' +\n\u001b[0;32m--> 162\u001b[0;31m                      str(identifier))\n\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m: Could not interpret initializer identifier: 500"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORePLkdaWKIb",
        "colab_type": "text"
      },
      "source": [
        "**Dot(axes) error**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfM4p1rsqAxz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#code_model.add(Convolution1D(filters=nb_filter,kernel_size=10,padding='valid',dilation_rate=1))\n",
        "#code_model.add(Activation('relu'))\n",
        "\n",
        "code_model = Convolution1D(filters=nb_filter,kernel_size=10,padding='valid',dilation_rate=1)(code_attention)\n",
        "code_model = Activation('relu')(code_attention)\n",
        "\n",
        "code_model = Convolution1D(filters=nb_filter,kernel_size=5,padding='valid',dilation_rate=1)(code_model)\n",
        "code_model = Activation('relu')(code_model)\n",
        "\n",
        "code_model = Convolution1D( filters=nb_filter,kernel_size=3,padding='valid',dilation_rate=1)(code_model)\n",
        "code_model = Activation('relu')(code_model)\n",
        "#def MaxPooling1D(pool_size=2, strides=None, padding='valid', data_format='channels_last', **kwargs)\n",
        "code_model = MaxPooling1D (pool_size=code_model.shape[1])(code_model)\n",
        "code_model = Flatten()(code_model)\n",
        "# We add a vanilla hidden layer:\n",
        "code_model = Dense(400)(code_model)\n",
        "\n",
        "\n",
        "t = Dense(400)(code_attention)\n",
        "\n",
        "h = Dense(400)(h)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzfhR8_1rfAj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7JJF2TEqHlD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = tf.keras.layers.Concatenate(axis=-1) (inputs=[t,h,mem_encoder])\n",
        "x = LSTM(400, return_sequences=False)(x)\n",
        "\n",
        "x = Dense(400)(x)\n",
        "\n",
        "x = Dropout(0.5)(x)\n",
        "predictions = Dense(nb_output,activation='softmax')(x)\n",
        "optimizer = RMSprop(lr=0.0001)\n",
        "model = tf.keras.Model ([code_input,language_input,vocab_input], predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Gx81vE2qMr3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile (loss='categorical_crossentropy', optimizer='rmsprop', metrics=[\"accuracy\"])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltle7yhkqNUv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 986
        },
        "outputId": "a9e2560d-2634-4dd1-90ad-e0c77d4157b8"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_4 (InputLayer)            [(None, 26)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_37 (Embedding)        (None, 26, 128)      640000      input_4[0][0]                    \n",
            "                                                                 input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "input_6 (InputLayer)            [(None, 600)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_18 (LSTM)                  (None, 26, 400)      846400      embedding_37[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "embedding_38 (Embedding)        (None, 600, 400)     4000000     input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "input_5 (InputLayer)            [(None, 26)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dot_6 (Dot)                     (None, 26, 600)      0           lstm_18[0][0]                    \n",
            "                                                                 embedding_38[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_53 (Activation)      (None, 26, 600)      0           dot_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dot_4 (Dot)                     (None, 26, 26)       0           embedding_37[0][0]               \n",
            "                                                                 embedding_37[1][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dot_7 (Dot)                     (None, 26, 400)      0           activation_53[0][0]              \n",
            "                                                                 embedding_38[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_51 (Activation)      (None, 26, 26)       0           dot_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_54 (Activation)      (None, 26, 400)      0           dot_7[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dot_5 (Dot)                     (None, 26, 128)      0           activation_51[0][0]              \n",
            "                                                                 embedding_37[1][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dense_42 (Dense)                (None, 26, 400)      160400      activation_54[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_43 (Dense)                (None, 26, 400)      160400      lstm_18[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_52 (Activation)      (None, 26, 128)      0           dot_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_9 (Concatenate)     (None, 26, 928)      0           dense_42[0][0]                   \n",
            "                                                                 dense_43[0][0]                   \n",
            "                                                                 activation_52[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "lstm_19 (LSTM)                  (None, 400)          2126400     concatenate_9[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_44 (Dense)                (None, 400)          160400      lstm_19[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_5 (Dropout)             (None, 400)          0           dense_44[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_45 (Dense)                (None, 3917)         1570717     dropout_5[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 9,664,717\n",
            "Trainable params: 9,664,717\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVDXEujEqNAx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c37a1c25-c8e9-4349-e4b8-07c689579412"
      },
      "source": [
        "print (\"train\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_O6ncNkgqM-Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "codesL, commentsL, raw_commentL = data_gen.getTestData()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msXV5gIE0dwi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "codes, partial_captions, next_words, predict_words= data_gen.MakeDataset4(True, mem_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fGMGNaNFtoL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "4909a8ca-76e4-43dd-c63b-62719611bcd0"
      },
      "source": [
        "print(codes)\n",
        "print(partial_captions )\n",
        "print(predict_words )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[]\n",
            "[]\n",
            "[]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5NVv4ZUxE_Go",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "codes = sequence.pad_sequences(codes, maxlen)\n",
        "partial_captions = sequence.pad_sequences(partial_captions, max_caption_len)\n",
        "predict_words = sequence.pad_sequences(predict_words, mem_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIPx42oJEtYv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "43908e9f-b84e-4f5c-8f96-dd3822a05c45"
      },
      "source": [
        "print(codes)\n",
        "print(partial_captions )\n",
        "print(predict_words )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[]\n",
            "[]\n",
            "[]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0n5NmrlqHzB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ids = data_gen.getTestID()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qcn6_hQgqeDr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "outputId": "37174470-e1c4-42c6-bc65-d26bd325048c"
      },
      "source": [
        "#def model.fit(x=None, y=None, batch_size=None, epochs=1, verbose=1, \n",
        " #             callbacks=None, validation_split=0.0, validation_data=None, shuffle=True, \n",
        "  #class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None, \n",
        "  #validation_batch_size=None, validation_freq=1, max_queue_size=10, workers=1, use_multiprocessing=False)\n",
        "#del data_gen\n",
        "\n",
        "epoch = 200\n",
        "for i in range(epoch):\n",
        "    model.fit ([codes, partial_captions,predict_words], next_words, batch_size=32, epochs=500,validation_data=([codesT, partial_captionsT,predict_wordsT], next_wordsT),callbacks=[early] )\n",
        "    json_string = model.to_json()\n",
        "    open('/content/drive/My Drive/CodeSum/dataset/attmem_qnaData_all.json', 'w').write(json_string)\n",
        "    model.save_weights('/content/drive/My Drive/CodeSum/dataset/attmem_qnaData_all.h5')\n",
        "     "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-198-de820710dc3f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartial_captions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpredict_words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcodesT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartial_captionsT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpredict_wordsT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_wordsT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mearly\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mjson_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/My Drive/CodeSum/dataset/attmem_qnaData_all.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1061\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1063\u001b[0;31m           steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[1;32m   1064\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[1;32m   1102\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_steps_per_execution_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msteps_per_execution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1104\u001b[0;31m     \u001b[0madapter_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_data_adapter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1105\u001b[0m     self._adapter = adapter_cls(\n\u001b[1;32m   1106\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mselect_data_adapter\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mselect_data_adapter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m   \u001b[0;34m\"\"\"Selects a data adapter than can handle a given x and y.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m   \u001b[0madapter_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcls\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mALL_ADAPTER_CLS\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcan_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    966\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0madapter_cls\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m     \u001b[0;31m# TODO(scottzhu): This should be a less implementation-specific error.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mselect_data_adapter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m   \u001b[0;34m\"\"\"Selects a data adapter than can handle a given x and y.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m   \u001b[0madapter_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcls\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mALL_ADAPTER_CLS\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcan_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    966\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0madapter_cls\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m     \u001b[0;31m# TODO(scottzhu): This should be a less implementation-specific error.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mcan_handle\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    617\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcan_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 619\u001b[0;31m     \u001b[0mhandles_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mListsOfScalarsDataAdapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_list_of_scalars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    620\u001b[0m     \u001b[0mhandles_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    621\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m_is_list_of_scalars\u001b[0;34m(inp)\u001b[0m\n\u001b[1;32m    628\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mListsOfScalarsDataAdapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_list_of_scalars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m_is_list_of_scalars\u001b[0;34m(inp)\u001b[0m\n\u001b[1;32m    628\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mListsOfScalarsDataAdapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_list_of_scalars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOc4DMKNqeS5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "outputId": "b6cf167b-a89b-4eb3-85bb-b7de4b027eff"
      },
      "source": [
        "json_string = model.to_json()\n",
        "open('../model/attmem_qnaData_all.json', 'w').write(json_string)\n",
        "model.save_weights('../model/attmem_qnaData_all.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-102-25672f30c025>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mjson_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../model/attmem_qnaData_all.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../model/attmem_qnaData_all.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../model/attmem_qnaData_all.json'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_ixYAAyalXX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#########not here!!!\n",
        "model.load_weights('/content/drive/My Drive/CodeSum/dataset/attmem_qnaData_all.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9uTmF9aRmuS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "outputId": "755c0935-56da-4727-b279-502f65f3eff2"
      },
      "source": [
        "    ids = data_gen.getTestID()\n",
        "    reference_file = \"ref_UNK.txt\"\n",
        "    predictions = []\n",
        "    for j, id in enumerate(ids):\n",
        "        predictions.append(str(id) + \"\\t\" + gen.printSentence(sens[j][0][1:-1]) + '\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-161-79f8f84e4c64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\t\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprintSentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuNI2e4Kqvku",
        "colab_type": "text"
      },
      "source": [
        "Training CodeNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BP4Oubw0qyMq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "d1117faf-46e2-4c54-c642-5b900c803582"
      },
      "source": [
        "import os\n",
        "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '2'\n",
        "import tensorflow.compat.v1 as tf\n",
        "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.8)\n",
        "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, allow_soft_placement=True, log_device_placement=True))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device mapping:\n",
            "/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpB-p5aUqyR2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.python.keras.models import Sequential\n",
        "from tensorflow.python.keras.layers.convolutional import Convolution1D\n",
        "from tensorflow.python.keras.layers.core import Activation, Flatten, Dense, Dropout, RepeatVector\n",
        "from tensorflow.python.keras.layers.pooling import MaxPooling1D\n",
        "from tensorflow.python.keras.optimizers import RMSprop\n",
        "from tensorflow.python.keras.layers import Input, LSTM, Embedding, Dense\n",
        "\n",
        "#import data_generator\n",
        "from tensorflow.python.keras.preprocessing import sequence\n",
        "#import keras\n",
        "\n",
        "from tensorflow.python.keras.models import Model\n",
        "\n",
        "#from bleu import computeMaps, bleuFromMaps\n",
        "#from text_generator import SentenceGeneration"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q65IswjXrL6n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_caption_len = 26\n",
        "maxlen = 500\n",
        "mem_size = 30\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5uZderjrMAg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5d845c30-2170-4ca0-fcf4-f2be2a232453"
      },
      "source": [
        "data_gen = DataGenerator(\"/content/drive/My Drive/CodeSum/dataset/code_f_keyword_indexed.txt\", \"/content/drive/My Drive/CodeSum/dataset/comment_f_indexed.txt\", 0.20,\n",
        "                                        maxlen, max_caption_len)\n",
        "\n",
        "codes, partial_captions, next_words = data_gen.MakeDataset3(train=True)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "num of data: 4885\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7q7oexQrMGV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "codes = sequence.pad_sequences(codes, maxlen=maxlen)\n",
        "partial_captions = sequence.pad_sequences(partial_captions, maxlen=max_caption_len)\n",
        "\n",
        "codesT, partial_captionsT, next_wordsT = data_gen.MakeDataset3(train=False)\n",
        "codesT = sequence.pad_sequences(codesT, maxlen=maxlen)\n",
        "partial_captionsT = sequence.pad_sequences(partial_captionsT, maxlen=max_caption_len)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihNpfEgKrMLf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size = 5000\n",
        "\n",
        "max_features = 10000  # size of token\n",
        "\n",
        "nb_output = data_gen.vocab_size  # len(Y_train[0])\n",
        "\n",
        "nb_filter = 128\n",
        "filter_length = 5\n",
        "hidden_dims = 512\n",
        "embedding_dims = 512\n",
        "\n",
        "mem_size = 30"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqdCj_1krMRL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_for_nse = Embedding(vocab_size,embedding_dims)\n",
        "###add dropout embedding_for_nse = Embedding(vocab_size,embedding_dims,dropout=0.3)\n",
        "language_input = Input(shape=(max_caption_len,))\n",
        "language_embedding = embedding_for_nse(language_input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLm9HBovrM0K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# the output of both models will be tensors of shape (samples, max_caption_len, 128).\n",
        "# let's encode this vector sequence into a single vector\n",
        "h = LSTM(hidden_dims, return_sequences=True)(language_embedding)\n",
        "# which will be used to compute a probability\n",
        "# distribution over what the next word in the caption should be!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbkC_QHOrMe6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "code_input = Input(shape=(maxlen,))\n",
        "embedding_for_code = Embedding(max_features, hidden_dims)(code_input)\n",
        "\n",
        "###add dropout embedding_for_nse = Embedding(vocab_size,embedding_dims,dropout=0.5)\n",
        "code_match = keras.layers.Dot(axes=[2,2]) (inputs=[h,embedding_for_code])\n",
        "code_match = Activation('softmax')(code_match)\n",
        "code_attention = keras.layers.Dot(axes=[2,1])(inputs=[code_match,embedding_for_code])\n",
        "code_attention = Activation('relu')(code_attention) # mem : memory embedding"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HONOHaFrMa8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t = Dense(hidden_dims)(code_attention)\n",
        "\n",
        "h = Dense(hidden_dims)(h)\n",
        "\n",
        "x = keras.layers.Concatenate(axis=-1) (inputs=[t,h])\n",
        "x = LSTM(hidden_dims, return_sequences=False)(x)\n",
        "\n",
        "x = Dense(hidden_dims)(x)\n",
        "\n",
        "x = Dropout(0.5)(x)\n",
        "predictions = Dense(nb_output,activation='softmax')(x)\n",
        "optimizer = RMSprop(lr=0.0001)\n",
        "model = Model([code_input,language_input], predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "duc5a6wYrMYD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=[\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcFT019Wr10Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 748
        },
        "outputId": "edd1b0e5-8826-4bfc-9620-9b40df2f3293"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 26)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_3 (Embedding)         (None, 26, 512)      2560000     input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 500)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     (None, 26, 512)      2099200     embedding_3[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "embedding_4 (Embedding)         (None, 500, 512)     5120000     input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dot (Dot)                       (None, 26, 500)      0           lstm[0][0]                       \n",
            "                                                                 embedding_4[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 26, 500)      0           dot[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "dot_1 (Dot)                     (None, 26, 512)      0           activation_15[0][0]              \n",
            "                                                                 embedding_4[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 26, 512)      0           dot_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 26, 512)      262656      activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_7 (Dense)                 (None, 26, 512)      262656      lstm[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 26, 1024)     0           dense_6[0][0]                    \n",
            "                                                                 dense_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 512)          3147776     concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_8 (Dense)                 (None, 512)          262656      lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_6 (Dropout)             (None, 512)          0           dense_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_9 (Dense)                 (None, 3917)         2009421     dropout_6[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 15,724,365\n",
            "Trainable params: 15,724,365\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHsgjCeMr2MD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1e80839e-dd55-4182-859d-76daae39c6cc"
      },
      "source": [
        "print (\"train\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WxmrNjMvNjK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "codesL, commentsL, raw_commentL = data_gen.getTestData()\n",
        "ids = data_gen.getTestID()\n",
        "\n",
        "del data_gen\n",
        "\n",
        "\n",
        "max_score = 0\n",
        "\n",
        "epoch = 200\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XeBlqVOYvNrf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "outputId": "c573acf8-2f5f-4e74-8974-d6062e796de4"
      },
      "source": [
        "#model.fit(x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None, \n",
        " # validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, \n",
        "#initial_epoch=0, steps_per_epoch=None, validation_steps=None, validation_batch_size=None, \n",
        "#validation_freq=1, max_queue_size=10, workers=1, use_multiprocessing=False)\n",
        "for i in range(epoch):\n",
        "    model.fit ([codesT, partial_captionsT], next_wordsT, batch_size=128, epochs=1, validation_split=0.5,validation_data=([codesT, partial_captionsT], next_wordsT),steps_per_epoch=2000,validation_steps=1)\n",
        "    json_string = model.to_json()\n",
        "    open('/content/drive/My Drive/CodeSum/dataset/codenn_512.json', 'w').write(json_string)\n",
        "    model.save_weights('/content/drive/My Drive/CodeSum/dataset/codenn_512.h5')\n",
        "\n",
        "\n",
        "    reference_file = \"ref_UNK.txt\"\n",
        "    predictions = []\n",
        "    for j, id in enumerate(ids):\n",
        "        predictions.append(str(id) + \"\\t\" + gen.printSentence(sens[j][0][1:-1]) + '\\n')\n",
        "    del gen\n",
        "    (goldMap, predictionMap) = computeMaps(predictions, reference_file)\n",
        "    score = bleuFromMaps(goldMap, predictionMap)[0]\n",
        "\n",
        "    print ('eter'+str(i))\n",
        "    print (score)\n",
        "\n",
        "    if max_score < score and i>5:\n",
        "        max_score = score\n",
        "        json_string = model.to_json()\n",
        "        open('/content/drive/My Drive/CodeSum/dataset/codenn_512.json', 'w').write(json_string)\n",
        "        model.save_weights('/content/drive/My Drive/CodeSum/dataset/codenn_512.h5')\n",
        "\n",
        "    print (max_score )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 258/2000 [==>...........................] - ETA: 2:17:50 - loss: 4.7433 - accuracy: 0.2520WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 2000 batches). You may need to use the repeat() function when building your dataset.\n",
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 2000 batches). You may need to use the repeat() function when building your dataset.\n",
            " 258/2000 [==>...........................] - 1227s 5s/step - loss: 4.7433 - accuracy: 0.2520 - val_loss: 4.3372 - val_accuracy: 0.2656\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-61-09b999a70d34>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\t\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprintSentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0mgoldMap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictionMap\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomputeMaps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreference_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-29-05d596a70e11>\u001b[0m in \u001b[0;36mprintSentence\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprintSentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex2word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m#for index in indices:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-29-05d596a70e11>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprintSentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex2word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m#for index in indices:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'e'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eh_m0WRgdATI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqGBP3_WvNot",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "   # del gen\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luoEw_p5ve0q",
        "colab_type": "text"
      },
      "source": [
        "BLEU Score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMlMhE_nZf_G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "324df110-e4e8-46fa-8ed7-804077049d28"
      },
      "source": [
        "#!/usr/bin/python\n",
        "\n",
        "'''\n",
        "This script was adapted from the original version by hieuhoang1972 which is part of MOSES.\n",
        "'''\n",
        "\n",
        "# $Id: bleu.py 1307 2007-03-14 22:22:36Z hieuhoang1972 $\n",
        "\n",
        "'''Provides:\n",
        "cook_refs(refs, n=4): Transform a list of reference sentences as strings into a form usable by cook_test().\n",
        "cook_test(test, refs, n=4): Transform a test sentence as a string (together with the cooked reference sentences) into a form usable by score_cooked().\n",
        "score_cooked(alltest, n=4): Score a list of cooked test sentences.\n",
        "score_set(s, testid, refids, n=4): Interface with dataset.py; calculate BLEU score of testid against refids.\n",
        "The reason for breaking the BLEU computation into three phases cook_refs(), cook_test(), and score_cooked() is to allow the caller to calculate BLEU scores for multiple test sets as efficiently as possible.\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Provides:\\ncook_refs(refs, n=4): Transform a list of reference sentences as strings into a form usable by cook_test().\\ncook_test(test, refs, n=4): Transform a test sentence as a string (together with the cooked reference sentences) into a form usable by score_cooked().\\nscore_cooked(alltest, n=4): Score a list of cooked test sentences.\\nscore_set(s, testid, refids, n=4): Interface with dataset.py; calculate BLEU score of testid against refids.\\nThe reason for breaking the BLEU computation into three phases cook_refs(), cook_test(), and score_cooked() is to allow the caller to calculate BLEU scores for multiple test sets as efficiently as possible.\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sryZ9pqvaZPu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys, math, re, xml.sax.saxutils\n",
        "import subprocess\n",
        "import os\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOkv5nbPxSW-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Added to bypass NIST-style pre-processing of hyp and ref files -- wade\n",
        "nonorm = 0\n",
        "\n",
        "preserve_case = False\n",
        "eff_ref_len = \"shortest\"\n",
        "\n",
        "normalize1 = [\n",
        "    ('<skipped>', ''),  # strip \"skipped\" tags\n",
        "    (r'-\\n', ''),  # strip end-of-line hyphenation and join lines\n",
        "    (r'\\n', ' '),  # join lines\n",
        "    #    (r'(\\d)\\s+(?=\\d)', r'\\1'), # join digits\n",
        "]\n",
        "normalize1 = [(re.compile(pattern), replace) for (pattern, replace) in normalize1]\n",
        "\n",
        "normalize2 = [\n",
        "    (r'([\\{-\\~\\[-\\` -\\&\\(-\\+\\:-\\@\\/])', r' \\1 '),  # tokenize punctuation. apostrophe is missing\n",
        "    (r'([^0-9])([\\.,])', r'\\1 \\2 '),  # tokenize period and comma unless preceded by a digit\n",
        "    (r'([\\.,])([^0-9])', r' \\1 \\2'),  # tokenize period and comma unless followed by a digit\n",
        "    (r'([0-9])(-)', r'\\1 \\2 ')  # tokenize dash when preceded by a digit\n",
        "]\n",
        "normalize2 = [(re.compile(pattern), replace) for (pattern, replace) in normalize2]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nFuONrGxStt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalize(s):\n",
        "    '''Normalize and tokenize text. This is lifted from NIST mteval-v11a.pl.'''\n",
        "    # Added to bypass NIST-style pre-processing of hyp and ref files -- wade\n",
        "    if (nonorm):\n",
        "        return s.split()\n",
        "    if type(s) is not str:\n",
        "        s = \" \".join(s)\n",
        "    # language-independent part:\n",
        "    for (pattern, replace) in normalize1:\n",
        "        s = re.sub(pattern, replace, s)\n",
        "    s = xml.sax.saxutils.unescape(s, {'&quot;': '\"'})\n",
        "    # language-dependent part (assuming Western languages):\n",
        "    s = \" %s \" % s\n",
        "    if not preserve_case:\n",
        "        s = s.lower()  # this might not be identical to the original\n",
        "    for (pattern, replace) in normalize2:\n",
        "        s = re.sub(pattern, replace, s)\n",
        "    return s.split()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZN46AoZCxS6w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def count_ngrams(words, n=4):\n",
        "    counts = {}\n",
        "    for k in xrange(1, n + 1):\n",
        "        for i in xrange(len(words) - k + 1):\n",
        "            ngram = tuple(words[i:i + k])\n",
        "            counts[ngram] = counts.get(ngram, 0) + 1\n",
        "    return counts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cGjuulaxTK8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cook_refs(refs, n=4):\n",
        "    '''Takes a list of reference sentences for a single segment\n",
        "    and returns an object that encapsulates everything that BLEU\n",
        "    needs to know about them.'''\n",
        "\n",
        "    refs = [normalize(ref) for ref in refs]\n",
        "    maxcounts = {}\n",
        "    for ref in refs:\n",
        "        counts = count_ngrams(ref, n)\n",
        "        for (ngram, count) in counts.iteritems():\n",
        "            maxcounts[ngram] = max(maxcounts.get(ngram, 0), count)\n",
        "    return ([len(ref) for ref in refs], maxcounts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXuMZOesxkoG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cook_test(test,reflens, refmaxcounts, n=4):\n",
        "    '''Takes a test sentence and returns an object that\n",
        "    encapsulates everything that BLEU needs to know about it.'''\n",
        "\n",
        "    test = normalize(test)\n",
        "    result = {}\n",
        "    result[\"testlen\"] = len(test)\n",
        "\n",
        "    # Calculate effective reference sentence length.\n",
        "\n",
        "    if eff_ref_len == \"shortest\":\n",
        "        result[\"reflen\"] = min(reflens)\n",
        "    elif eff_ref_len == \"average\":\n",
        "        result[\"reflen\"] = float(sum(reflens)) / len(reflens)\n",
        "    elif eff_ref_len == \"closest\":\n",
        "        min_diff = None\n",
        "        for reflen in reflens:\n",
        "            if min_diff is None or abs(reflen - len(test)) < min_diff:\n",
        "                min_diff = abs(reflen - len(test))\n",
        "                result['reflen'] = reflen\n",
        "\n",
        "    result[\"guess\"] = [max(len(test) - k + 1, 0) for k in xrange(1, n + 1)]\n",
        "\n",
        "    result['correct'] = [0] * n\n",
        "    counts = count_ngrams(test, n)\n",
        "    for (ngram, count) in counts.iteritems():\n",
        "        result[\"correct\"][len(ngram) - 1] += min(refmaxcounts.get(ngram, 0), count)\n",
        "\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EigS_lsXxk16",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def score_cooked(allcomps, n=4, ground=0, smooth=1):\n",
        "    totalcomps = {'testlen': 0, 'reflen': 0, 'guess': [0] * n, 'correct': [0] * n}\n",
        "    for comps in allcomps:\n",
        "        for key in ['testlen', 'reflen']:\n",
        "            totalcomps[key] += comps[key]\n",
        "        for key in ['guess', 'correct']:\n",
        "            for k in xrange(n):\n",
        "                totalcomps[key][k] += comps[key][k]\n",
        "    logbleu = 0.0\n",
        "    all_bleus = []\n",
        "    for k in xrange(n):\n",
        "        correct = totalcomps['correct'][k]\n",
        "        guess = totalcomps['guess'][k]\n",
        "        addsmooth = 0\n",
        "        if smooth == 1 and k > 0:\n",
        "            addsmooth = 1\n",
        "        logbleu += math.log(correct + addsmooth + sys.float_info.min) - math.log(guess + addsmooth)\n",
        "        if guess == 0:\n",
        "            all_bleus.append(-10000000)\n",
        "        else:\n",
        "            all_bleus.append(math.log(correct + sys.float_info.min) - math.log(guess))\n",
        "\n",
        "    logbleu /= float(n)\n",
        "    all_bleus.insert(0, logbleu)\n",
        "\n",
        "    brevPenalty = min(0, 1 - float(totalcomps['reflen'] + 1) / (totalcomps['testlen'] + 1))\n",
        "    for i in xrange(len(all_bleus)):\n",
        "        if i == 0:\n",
        "            all_bleus[i] += brevPenalty\n",
        "        all_bleus[i] = math.exp(all_bleus[i])\n",
        "    return all_bleus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4g6L9fspxlA9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bleu(refs, candidate, ground=0, smooth=1):\n",
        "    refs = cook_refs(refs)\n",
        "    test = cook_test(candidate, refs)\n",
        "    return score_cooked([test], ground=ground, smooth=smooth)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v69E3k8hxlMM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def splitPuncts(line):\n",
        "    return ' '.join(re.findall(r\"[\\w]+|[^\\s\\w]\", line))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCHkGR9YxlVd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def computeMaps(predictions, goldfile):\n",
        "    predictionMap = {}\n",
        "    goldMap = {}\n",
        "    gf = open(goldfile, 'r')\n",
        "\n",
        "    for row in predictions:\n",
        "        cols = row.strip().split('\\t')\n",
        "        if len(cols) == 1:\n",
        "            (rid, pred) = (cols[0], '')\n",
        "        else:\n",
        "            (rid, pred) = (cols[0], cols[1])\n",
        "        predictionMap[rid] = [splitPuncts(pred.strip().lower())]\n",
        "\n",
        "    for row in gf:\n",
        "        (rid, pred) = row.split('\\t')\n",
        "        if rid in predictionMap:  # Only insert if the id exists for the method\n",
        "            if rid not in goldMap:\n",
        "                goldMap[rid] = []\n",
        "            goldMap[rid].append(splitPuncts(pred.strip().lower()))\n",
        "\n",
        "    sys.stderr.write('Total: ' + str(len(goldMap)) + '\\n')\n",
        "    return (goldMap, predictionMap)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKFiOvvrxlgR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# m1 is the reference map\n",
        "# m2 is the prediction map\n",
        "def bleuFromMaps(m1, m2):\n",
        "    score = [0] * 5\n",
        "    num = 0.0\n",
        "\n",
        "    for key in m1:\n",
        "        if key in m2:\n",
        "            bl = bleu(m1[key], m2[key][0])\n",
        "            score = [score[i] + bl[i] for i in range(0, len(bl))]\n",
        "            num += 1\n",
        "    return [s * 100.0 / num for s in score]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OezRQagx_tK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "outputId": "7418733f-8eb8-419b-aa00-d461bd4f7986"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "\n",
        "    file = 'predict_1_codenn_512'\n",
        "\n",
        "    ids = []\n",
        "    with open('/content/drive/My Drive/CodeSum/codenn/data/stackoverflow/csharp/dev/codenn.txt') as f:\n",
        "        ids = [val.split('\\t')[0] for val in f]\n",
        "\n",
        "    reference_file = \"/content/drive/My Drive/CodeSum/generator/ref.txt\"\n",
        "    predictions = []\n",
        "    for i,row in enumerate(open(file+'.txt')):\n",
        "        r = str(ids[i])+\"\\t\"+row.split('\\t')[1]\n",
        "        predictions.append(r)\n",
        "    (goldMap, predictionMap) = computeMaps(predictions, reference_file)\n",
        "    print (bleuFromMaps(goldMap, predictionMap)[0])\n",
        "    print (bleuFromMaps(goldMap, predictionMap)[1])\n",
        "    print (bleuFromMaps(goldMap, predictionMap)[2])\n",
        "    print (bleuFromMaps(goldMap, predictionMap)[3])\n",
        "    print (bleuFromMaps(goldMap, predictionMap)[4])\n",
        "\n",
        "    reference_file = \"ref_UNK.txt\"\n",
        "    predictions = []\n",
        "    for i, row in enumerate(open(file+'.txt')):\n",
        "        r = str(ids[i]) + \"\\t\" + row.split('\\t')[1]\n",
        "        predictions.append(r)\n",
        "    (goldMap, predictionMap) = computeMaps(predictions, reference_file)\n",
        "    print (bleuFromMaps(goldMap, predictionMap)[0])\n",
        "    print (bleuFromMaps(goldMap, predictionMap)[1])\n",
        "    print (bleuFromMaps(goldMap, predictionMap)[2])\n",
        "    print (bleuFromMaps(goldMap, predictionMap)[3])\n",
        "    print (bleuFromMaps(goldMap, predictionMap)[4])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total: 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "ZeroDivisionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-83-1957ee7b89a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0mgoldMap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictionMap\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomputeMaps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreference_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbleuFromMaps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgoldMap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictionMap\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbleuFromMaps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgoldMap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictionMap\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbleuFromMaps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgoldMap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictionMap\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-65-f4d54d0dec40>\u001b[0m in \u001b[0;36mbleuFromMaps\u001b[0;34m(m1, m2)\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mnum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100.0\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnum\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-65-f4d54d0dec40>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mnum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100.0\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnum\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mZeroDivisionError\u001b[0m: float division by zero"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptyvAr73zH1Z",
        "colab_type": "text"
      },
      "source": [
        "Text Generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nOc0SOgx_8c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "3b342d71-1b5f-486c-82a3-4c990b66c4dc"
      },
      "source": [
        "import os\n",
        "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '2'\n",
        "import tensorflow.compat.v1 as tf\n",
        "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.1)\n",
        "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, allow_soft_placement=True, log_device_placement=True))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device mapping:\n",
            "/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYtgQL6pyAKM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.python.keras.models import model_from_json, Sequential\n",
        "import numpy as np\n",
        "import math\n",
        "import logging\n",
        "import sys\n",
        "import nltk\n",
        "from tensorflow.python.keras.models import Sequential\n",
        "#import data_generator\n",
        "from tensorflow.python.keras.preprocessing import sequence\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KomZIoaJyAVN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logging.basicConfig(level=logging.INFO, stream=sys.stdout)\n",
        "logger = logging.getLogger(__name__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Ipp2l_VyAht",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SentenceGeneration(object):\n",
        "    def __init__(self):\n",
        "       # pp=CGModel(20000, 600, 100, 200, 4000 , 'rmsprop', None,False, -1, None,None, 0.001)\n",
        "        self.model =  Sequential()              #pp.buildModel() \n",
        "        self.index2word = dict()\n",
        "        self.word2Index = dict()\n",
        "        self.index2token = dict()\n",
        "        self.token2Index = dict()\n",
        "\n",
        "        self.predict_model = self.readKeywordModel('keyword_f')\n",
        "        ### in place keyword_f q_keyword2\n",
        "\n",
        "        with open(\"/content/drive/My Drive/CodeSum/dataset/comment_f_Vocab.txt\") as fin:\n",
        "            for i, word in enumerate(fin):\n",
        "                word = word.rstrip()\n",
        "                self.index2word[i] = word\n",
        "                self.word2Index[word] = int(i)\n",
        "        with open(\"/content/drive/My Drive/CodeSum/dataset/code_f_keyword_Vocab.txt\") as fin:\n",
        "            for i, word in enumerate(fin):\n",
        "                word = word.rstrip()\n",
        "                self.index2token[i] = word\n",
        "                self.token2Index[word] = int(i)\n",
        "\n",
        "    # def __init__(self,codes,targets):\n",
        "    #     self.model = Sequential()\n",
        "    #     # self.codes = codes\n",
        "    #     # self.targets = targets\n",
        "\n",
        "    def readModel(self, name):\n",
        "        model = model_from_json(open('/content/drive/My Drive/CodeSum/dataset/'+name + '.json').read())\n",
        "        model.load_weights('/content/drive/My Drive/CodeSum/dataset/'+name + '.h5')\n",
        "        model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=[\"accuracy\"])\n",
        "        self.model = model\n",
        "\n",
        "    def readKeywordModel(self, name):\n",
        "        model = model_from_json(open('/content/drive/My Drive/CodeSum/dataset/' + name + '.json').read())\n",
        "        model.load_weights('/content/drive/My Drive/CodeSum/dataset/' + name + '.h5')\n",
        "        model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=[\"accuracy\"])\n",
        "        return model\n",
        "\n",
        "    def setModel(self, model):\n",
        "        self.model = model\n",
        "\n",
        "    def printSentence(self, indices):\n",
        "        return ' '.join([self.index2word[x] for x in indices])\n",
        "\n",
        "        #for index in indices:\n",
        "        #    print self.index2word[index],\n",
        "    def returnCode(self, indices):\n",
        "\n",
        "        return ' '.join([self.index2token[x-1] for x in indices])\n",
        "\n",
        "    def removeToken(self,indices):\n",
        "        indices = indices[1:-1]\n",
        "        unk_index = self.word2Index['UNK']\n",
        "        if unk_index in indices:\n",
        "            indices.remove(unk_index)\n",
        "        return indices\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def generateSentence(self, code,beam_size,nonUNK,useMemory):\n",
        "\n",
        "        code = sequence.pad_sequences([code], maxlen=500)\n",
        "\n",
        "        if useMemory:\n",
        "            keyword_pred = self.predict_model.model.predict(code)[0]\n",
        "\n",
        "            keyword_sorted_pred = np.argsort(keyword_pred)[::-1]\n",
        "            keywords = [s for s in keyword_sorted_pred[:30]]\n",
        "            keywords = sequence.pad_sequences([keywords], maxlen=30)\n",
        "\n",
        "\n",
        "        start_index = self.word2Index['START']\n",
        "        end_index = self.word2Index['END']\n",
        "        caption = [[start_index]]\n",
        "        caption = sequence.pad_sequences(caption, maxlen=26)\n",
        "\n",
        "        if useMemory:\n",
        "            pred = self.model.predict([code, caption, keywords])[0]\n",
        "        else:\n",
        "            pred = self.model.predict([code, caption])[0]\n",
        "        sorted_pred = np.argsort(pred)[::-1]\n",
        "\n",
        "        # initial\n",
        "        beams = []\n",
        "        for index in sorted_pred[:beam_size]:\n",
        "            beams.append((math.log(pred[index]), [start_index, index]))\n",
        "\n",
        "        finished = []\n",
        "\n",
        "        for t in range(1, 26):  # max_caption_len\n",
        "            # generate\n",
        "            candidates = []\n",
        "            for b in beams:\n",
        "                caption = sequence.pad_sequences([b[1]], maxlen=26)\n",
        "                if useMemory:\n",
        "                    pred = self.model.predict([code, caption, keywords])[0]\n",
        "                else:\n",
        "                    pred = self.model.predict([code, caption])[0]\n",
        "                sorted_pred = np.argsort(pred)[::-1]\n",
        "\n",
        "                if nonUNK:\n",
        "                    candi_index = [val for val in sorted_pred[:beam_size]]\n",
        "                    if self.word2Index['UNK'] in candi_index:\n",
        "                        candi_index.append(sorted_pred[beam_size+1])\n",
        "                else:\n",
        "                    candi_index = sorted_pred[:beam_size]\n",
        "\n",
        "\n",
        "                for index in candi_index:\n",
        "                    if nonUNK:\n",
        "                        if self.word2Index['UNK'] == index:\n",
        "                            continue\n",
        "\n",
        "                    new_caption = b[1][:]\n",
        "                    new_caption.append(index)\n",
        "                    candidates.append([b[0] + math.log(pred[index]), new_caption])\n",
        "            candidates.sort(reverse=True)\n",
        "            beams = candidates[:beam_size]\n",
        "\n",
        "            # end sentence\n",
        "            unfinished = []\n",
        "            for b in beams:\n",
        "                if b[1][-1] == end_index:\n",
        "                    finished.append(b)\n",
        "                    beam_size -= 1\n",
        "                else:\n",
        "                    unfinished.append(b)\n",
        "            beams = unfinished[:]\n",
        "\n",
        "            # print \"\"\n",
        "            # for b in beams:\n",
        "            #     print \"%5f\" % b[0], ' '.join([self.index2word[key] for key in b[1]])\n",
        "\n",
        "            if beam_size == 0:\n",
        "                break\n",
        "\n",
        "        for i in range(beam_size):\n",
        "            finished.append(beams[i])\n",
        "\n",
        "        # re-ranking\n",
        "        for f in finished:\n",
        "            f[0] = f[0] / len(f[1])\n",
        "            finished.sort(reverse=True)\n",
        "\n",
        "        # print \"---generate sentences---\"\n",
        "        s = []\n",
        "        for b in finished:\n",
        "            #sen = ' '.join([self.index2word[key] for key in b[1]])\n",
        "            s.append(b[1])\n",
        "        return s"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pf6agaloDmhB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from textgenrnn import textgenrnn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "la2vcO-iEgiE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HguE5A9lHKuh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "b005f752-185d-4d1e-b115-5adfc4e41f31"
      },
      "source": [
        "!pip install textgenrnn"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: textgenrnn in /usr/local/lib/python3.6/dist-packages (1.4.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from textgenrnn) (2.10.0)\n",
            "Requirement already satisfied: keras>=2.1.5 in /usr/local/lib/python3.6/dist-packages (from textgenrnn) (2.4.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from textgenrnn) (0.22.2.post1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->textgenrnn) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from h5py->textgenrnn) (1.18.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras>=2.1.5->textgenrnn) (3.13)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras>=2.1.5->textgenrnn) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->textgenrnn) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y29-uy41yAtj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        },
        "outputId": "8fafcabc-c5ac-4b14-fe32-b74512d8ee47"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    gen = SentenceGeneration()\n",
        "    gen.readModel('codenn_512')\n",
        "    #### replace keyword_f with codenn_512\n",
        "    name = 'codenn_512'\n",
        "    nonUNK = 1\n",
        "    beamSize = 10\n",
        "    useMemory = 1\n",
        "\n",
        "    data_gen = DataGenerator(\"/content/drive/My Drive/CodeSum/dataset/code_f_keyword_indexed.txt\", \"/content/drive/My Drive/CodeSum/dataset/comment_f_indexed.txt\",\n",
        "                                            0.20, 500, 26)\n",
        "\n",
        "    codes,comments,raw_comment= data_gen.getTestData()\n",
        "    '''\n",
        "    np.random.seed(30)\n",
        "    np.random.shuffle(codes)\n",
        "    np.random.seed(30)\n",
        "    np.random.shuffle(raw_comment)\n",
        "    np.random.seed(30)\n",
        "    np.random.shuffle(comments)\n",
        "    '''\n",
        "    sum_of_BLEU1 = 0\n",
        "    sum_of_BLEU2 = 0\n",
        "    sum_of_BLEU3 = 0\n",
        "    sum_of_BLEU4 = 0\n",
        "    j = 0\n",
        "\n",
        "    sens = []\n",
        "    co = []\n",
        "    comm = []\n",
        "    raw_com=[]\n",
        "\n",
        "    score = []\n",
        "\n",
        "    for code,comment,raw in zip(codes,comments,raw_comment):\n",
        "        print(code)\n",
        "        sentences = gen.generateSentence(code,beamSize,nonUNK,0)\n",
        "\n",
        "\n",
        "\n",
        "        sens.append(sentences)\n",
        "        co.append(gen.returnCode(code))\n",
        "        comm.append(gen.printSentence(comment[1:-1]))\n",
        "        raw_com.append(raw)\n",
        "        #print \"origen\"\n",
        "        #print gen.printSentence(comment)\n",
        "        #print \"generate\"\n",
        "        #for s in sentences:\n",
        "        #    print gen.printSentence(s)\n",
        "        #print comment\n",
        "        #print sentences[0]\n",
        "        # s = gen.removeToken(sentences[0])\n",
        "        # c = gen.removeToken(comment)\n",
        "        s = sentences[0][1:-1]\n",
        "        c = comment[1:-1]\n",
        "\n",
        "        BLEU1_score = nltk.translate.bleu_score.modified_precision([c], s, n=1)\n",
        "        BLEU2_score = nltk.translate.bleu_score.modified_precision([c], s, n=2)\n",
        "        BLEU3_score = nltk.translate.bleu_score.modified_precision([c], s, n=3)\n",
        "        BLEU4_score = nltk.translate.bleu_score.modified_precision([c], s, n=4)\n",
        "\n",
        "        sum_of_BLEU1 += float(BLEU1_score)\n",
        "        sum_of_BLEU2 += float(BLEU2_score)\n",
        "        sum_of_BLEU3 += float(BLEU3_score)\n",
        "        sum_of_BLEU4 += float(BLEU4_score)\n",
        "        score.append([BLEU1_score,j])\n",
        "        j += 1\n",
        "\n",
        "\n",
        "        sys.stdout.write('\\r' + str(j)+' score :'+str(sum_of_BLEU1 / float(j)))\n",
        "\n",
        "\n",
        "        #print 'score :', sum_of_BLEU / float(i)\n",
        "        #print i,float(BLEU_score)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "num of data: 4885\n",
            "[1, 2, 9, 10, 11, 12, 6, 13, 11, 12, 14, 13, 15, 7, 8]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-79-4897181fd6ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcomment\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mraw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcodes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcomments\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mraw_comment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerateSentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbeamSize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnonUNK\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-77-7fce7852a24a>\u001b[0m in \u001b[0;36mgenerateSentence\u001b[0;34m(self, code, beam_size, nonUNK, useMemory)\u001b[0m\n\u001b[1;32m    100\u001b[0m                     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaption\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeywords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m                     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaption\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m                 \u001b[0msorted_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m       raise ValueError('{} is not supported in multi-worker mode.'.format(\n\u001b[1;32m    129\u001b[0m           method.__name__))\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m   return tf_decorator.make_decorator(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1597\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1598\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1599\u001b[0;31m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1600\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1601\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    812\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    815\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGjXI495bbxL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for code,comment,raw in zip(codes,comments,raw_comment)[:]:\n",
        "        sentences = gen.generateSentence(code,beamSize,nonUNK=nonUNK,useMemory=0)\n",
        "\n",
        "\n",
        "\n",
        "        sens.append(sentences)\n",
        "        co.append(gen.returnCode(code))\n",
        "        comm.append(gen.printSentence(comment[1:-1]))\n",
        "        raw_com.append(raw)\n",
        "        #print \"origen\"\n",
        "        #print gen.printSentence(comment)\n",
        "        #print \"generate\"\n",
        "        #for s in sentences:\n",
        "        #    print gen.printSentence(s)\n",
        "        #print comment\n",
        "        #print sentences[0]\n",
        "        # s = gen.removeToken(sentences[0])\n",
        "        # c = gen.removeToken(comment)\n",
        "        s = sentences[0][1:-1]\n",
        "        c = comment[1:-1]\n",
        "\n",
        "        BLEU1_score = nltk.translate.bleu_score.modified_precision([c], s, n=1)\n",
        "        BLEU2_score = nltk.translate.bleu_score.modified_precision([c], s, n=2)\n",
        "        BLEU3_score = nltk.translate.bleu_score.modified_precision([c], s, n=3)\n",
        "        BLEU4_score = nltk.translate.bleu_score.modified_precision([c], s, n=4)\n",
        "\n",
        "        sum_of_BLEU1 += float(BLEU1_score)\n",
        "        sum_of_BLEU2 += float(BLEU2_score)\n",
        "        sum_of_BLEU3 += float(BLEU3_score)\n",
        "        sum_of_BLEU4 += float(BLEU4_score)\n",
        "        score.append([BLEU1_score,j])\n",
        "        j += 1\n",
        "\n",
        "\n",
        "        sys.stdout.write('\\r' + str(j)+' score :'+str(sum_of_BLEU1 / float(j)))\n",
        "\n",
        "\n",
        "        #print 'score :', sum_of_BLEU / float(i)\n",
        "        #print i,float(BLEU_score)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    score.sort(reverse=True)\n",
        "    #high_score_index = [index[1] for index in score]\n",
        "\n",
        "    with open('code_rand_'+name+'.txt', 'w') as fin:\n",
        "        fin.write(str(sum_of_BLEU1 / float(j)))\n",
        "        fin.write(\"\\n\")\n",
        "        fin.write(str(sum_of_BLEU2 / float(j)))\n",
        "        fin.write(\"\\n\")\n",
        "        fin.write(str(sum_of_BLEU3 / float(j)))\n",
        "        fin.write(\"\\n\")\n",
        "        fin.write(str(sum_of_BLEU4 / float(j)))\n",
        "        fin.write(\"\\n\")\n",
        "        for i in range(len(co[:])):\n",
        "            s = str(co[i])\n",
        "            code = s.replace(\"{\",\"{\\n\").replace(\";\",\";\\n\").replace(\"}\",\"}\\n\")\n",
        "            fin.write(\"code:\\n\"+ code)\n",
        "            fin.write(\"comment:\\n\"+ comm[i].rstrip()+ '\\n')\n",
        "            fin.write(\"--generate--\\n\")\n",
        "            for s in sens[i]:\n",
        "                fin.write(gen.printSentence(s[1:-1])+'\\n')\n",
        "            fin.write(\"\\n\")\n",
        "\n",
        "    with open('code_sort_'+name+'.txt', 'w') as fin:\n",
        "        fin.write(str(sum_of_BLEU1 / float(j)))\n",
        "        fin.write(\"\\n\")\n",
        "        fin.write(str(sum_of_BLEU2 / float(j)))\n",
        "        fin.write(\"\\n\")\n",
        "        fin.write(str(sum_of_BLEU3 / float(j)))\n",
        "        fin.write(\"\\n\")\n",
        "        fin.write(str(sum_of_BLEU4 / float(j)))\n",
        "        fin.write(\"\\n\")\n",
        "\n",
        "        for (sc,i) in score:\n",
        "            s = str(co[i])\n",
        "            code = s.replace(\"{\", \"{\\n\").replace(\";\", \";\\n\").replace(\"}\", \"}\\n\")\n",
        "            fin.write(\"code:\\n\" + code)\n",
        "            fin.write(\"comment:\\n\" + comm[i].rstrip() + '\\n')\n",
        "            fin.write(\"BLUE1:\"+ str(float(sc))+ '\\n')\n",
        "            fin.write(\"--generate--\\n\")\n",
        "            for s in sens[i]:\n",
        "                fin.write(gen.printSentence(s[1:-1]) + '\\n')\n",
        "            fin.write(\"\\n\")\n",
        "\n",
        "\n",
        "    with open('gold.txt', 'w') as fin:\n",
        "        for i in range(len(co[:])):\n",
        "            fin.write(str(i)+\"\\t\"+comm[i].rstrip()+ '\\n')\n",
        "    with open('predict_n_'+name+'.txt', 'w') as fin:\n",
        "        for i in range(len(co[:])):\n",
        "            for s in sens[i]:\n",
        "                fin.write(str(i)+\"\\t\"+gen.printSentence(s[1:-1])+'\\n')\n",
        "\n",
        "\n",
        "    with open('predict_1_'+name+'.txt', 'w') as fin:\n",
        "        for i in range(len(co[:])):\n",
        "            fin.write(str(i)+\"\\t\"+gen.printSentence(sens[i][0][1:-1])+'\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SveN4pprbnK5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "287f1f2d-2247-4744-ed3e-179a738a7687"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    gen = SentenceGeneration()\n",
        "    gen.readModel('codenn_512')\n",
        "    name = 'codenn_512'\n",
        "    nonUNK = 1\n",
        "    beamSize = 10\n",
        "    useMemory = 1\n",
        "\n",
        "    data_gen = DataGenerator(\"/content/drive/My Drive/CodeSum/dataset/code_f_keyword_indexed.txt\", \"/content/drive/My Drive/CodeSum/dataset/comment_f_indexed.txt\",\n",
        "                                            0.20, 500, 26)\n",
        "\n",
        "    codes,comments,raw_comment= data_gen.getTestData()\n",
        "    '''\n",
        "    np.random.seed(30)\n",
        "    np.random.shuffle(codes)\n",
        "    np.random.seed(30)\n",
        "    np.random.shuffle(raw_comment)\n",
        "    np.random.seed(30)\n",
        "    np.random.shuffle(comments)\n",
        "    '''\n",
        "    sum_of_BLEU1 = 0\n",
        "    sum_of_BLEU2 = 0\n",
        "    sum_of_BLEU3 = 0\n",
        "    sum_of_BLEU4 = 0\n",
        "    j = 0\n",
        "\n",
        "    sens = []\n",
        "    co = []\n",
        "    comm = []\n",
        "    raw_com=[]\n",
        "\n",
        "    score = []\n",
        "\n",
        "    for code,comment,raw in zip(codes,comments,raw_comment):\n",
        "        sentences = gen.generateSentence(code,beamSize,nonUNK=nonUNK,useMemory=0)\n",
        "\n",
        "\n",
        "\n",
        "        sens.append(sentences)\n",
        "        co.append(gen.returnCode(code))\n",
        "        comm.append(gen.printSentence(comment[1:-1]))\n",
        "        raw_com.append(raw)\n",
        "        #print \"origen\"\n",
        "        #print gen.printSentence(comment)\n",
        "        #print \"generate\"\n",
        "        #for s in sentences:\n",
        "        #    print gen.printSentence(s)\n",
        "        #print comment\n",
        "        #print sentences[0]\n",
        "        # s = gen.removeToken(sentences[0])\n",
        "        # c = gen.removeToken(comment)\n",
        "        s = sentences[0][1:-1]\n",
        "        c = comment[1:-1]\n",
        "\n",
        "        BLEU1_score = nltk.translate.bleu_score.modified_precision([c], s, n=1)\n",
        "        BLEU2_score = nltk.translate.bleu_score.modified_precision([c], s, n=2)\n",
        "        BLEU3_score = nltk.translate.bleu_score.modified_precision([c], s, n=3)\n",
        "        BLEU4_score = nltk.translate.bleu_score.modified_precision([c], s, n=4)\n",
        "\n",
        "        sum_of_BLEU1 += float(BLEU1_score)\n",
        "        sum_of_BLEU2 += float(BLEU2_score)\n",
        "        sum_of_BLEU3 += float(BLEU3_score)\n",
        "        sum_of_BLEU4 += float(BLEU4_score)\n",
        "        score.append([BLEU1_score,j])\n",
        "        j += 1\n",
        "\n",
        "\n",
        "        sys.stdout.write('\\r' + str(j)+' score :'+str(sum_of_BLEU1 / float(j)))\n",
        "\n",
        "\n",
        "        #print 'score :', sum_of_BLEU / float(i)\n",
        "        #print i,float(BLEU_score)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    score.sort(reverse=True)\n",
        "    #high_score_index = [index[1] for index in score]\n",
        "\n",
        "    with open('/content/drive/My Drive/CodeSum/dataset/code_rand_'+name+'.txt', 'w') as fin:\n",
        "        fin.write(str(sum_of_BLEU1 / float(j)))\n",
        "        fin.write(\"\\n\")\n",
        "        fin.write(str(sum_of_BLEU2 / float(j)))\n",
        "        fin.write(\"\\n\")\n",
        "        fin.write(str(sum_of_BLEU3 / float(j)))\n",
        "        fin.write(\"\\n\")\n",
        "        fin.write(str(sum_of_BLEU4 / float(j)))\n",
        "        fin.write(\"\\n\")\n",
        "        for i in range(len(co[:])):\n",
        "            s = str(co[i])\n",
        "            code = s.replace(\"{\",\"{\\n\").replace(\";\",\";\\n\").replace(\"}\",\"}\\n\")\n",
        "            fin.write(\"code:\\n\"+ code)\n",
        "            fin.write(\"comment:\\n\"+ comm[i].rstrip()+ '\\n')\n",
        "            fin.write(\"--generate--\\n\")\n",
        "            for s in sens[i]:\n",
        "                fin.write(gen.printSentence(s[1:-1])+'\\n')\n",
        "            fin.write(\"\\n\")\n",
        "\n",
        "    with open('/content/drive/My Drive/CodeSum/dataset/code_sort_'+name+'.txt', 'w') as fin:\n",
        "        fin.write(str(sum_of_BLEU1 / float(j)))\n",
        "        fin.write(\"\\n\")\n",
        "        fin.write(str(sum_of_BLEU2 / float(j)))\n",
        "        fin.write(\"\\n\")\n",
        "        fin.write(str(sum_of_BLEU3 / float(j)))\n",
        "        fin.write(\"\\n\")\n",
        "        fin.write(str(sum_of_BLEU4 / float(j)))\n",
        "        fin.write(\"\\n\")\n",
        "\n",
        "        for (sc,i) in score:\n",
        "            s = str(co[i])\n",
        "            code = s.replace(\"{\", \"{\\n\").replace(\";\", \";\\n\").replace(\"}\", \"}\\n\")\n",
        "            fin.write(\"code:\\n\" + code)\n",
        "            fin.write(\"comment:\\n\" + comm[i].rstrip() + '\\n')\n",
        "            fin.write(\"BLUE1:\"+ str(float(sc))+ '\\n')\n",
        "            fin.write(\"--generate--\\n\")\n",
        "            for s in sens[i]:\n",
        "                fin.write(gen.printSentence(s[1:-1]) + '\\n')\n",
        "            fin.write(\"\\n\")\n",
        "\n",
        "\n",
        "    with open('/content/drive/My Drive/CodeSum/dataset/gold.txt', 'w') as fin:\n",
        "        for i in range(len(co[:])):\n",
        "            fin.write(str(i)+\"\\t\"+comm[i].rstrip()+ '\\n')\n",
        "    with open('/content/drive/My Drive/CodeSum/dataset/predict_n_'+name+'.txt', 'w') as fin:\n",
        "        for i in range(len(co[:])):\n",
        "            for s in sens[i]:\n",
        "                fin.write(str(i)+\"\\t\"+gen.printSentence(s[1:-1])+'\\n')\n",
        "\n",
        "\n",
        "    with open('/content/drive/My Drive/CodeSum/dataset/predict_1_'+name+'.txt', 'w') as fin:\n",
        "        for i in range(len(co[:])):\n",
        "            fin.write(str(i)+\"\\t\"+gen.printSentence(sens[i][0][1:-1])+'\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "num of data: 4885\n",
            "489 score :0.3100715746421268"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}